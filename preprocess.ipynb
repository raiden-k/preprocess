{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import docx\n",
    "from pptx import Presentation\n",
    "import xlrd\n",
    "import docx\n",
    "#import \n",
    "\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './input/ファイル1.pptx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slide_name= 0\n",
      "タイトル\n",
      "あいうえお\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prs = Presentation('./input/ファイル1.pptx')\n",
    "except:\n",
    "    ##　パスワード付きなどで開けないとエラーになる。\n",
    "    ##pptx.exc.PackageNotFoundError: Package not found at  ファイル名\n",
    "    sys.exit()\n",
    "\n",
    "#中に入っているslidesの数分の繰り返し\n",
    "for islide in range(0, len(prs.slides)):\n",
    "    #スライド名出力　（１階層目）\n",
    "    print(\"slide_name=\",str(islide))\n",
    "    #print (\"\\tfor slide => \" + str(islide))\n",
    "    #python-pptxとしてのスライドを取得\n",
    "    slide = prs.slides[islide]\n",
    "    #スライドのshapesの分だけ繰り返す。\n",
    "    for shape in slide.shapes:\n",
    "        #テーブル構造を持っている場合は、\n",
    "        #if shape.has_table:などで分岐して別処理が必要。\n",
    "        #長くなるのでここでは省略。\n",
    "        shapeText=\"\"\n",
    "        #テキスト構造を持たないshapesは無視して次に進む。\n",
    "        if not shape.has_text_frame:\n",
    "            continue\n",
    "        for paragraph in shape.text_frame.paragraphs:\n",
    "            #.strip()は、改行コードやタブ情報などを削除する。\n",
    "            shapeText += paragraph.text.strip().replace('\\n','').replace('\\r','')\n",
    "        #もし、そのshape内にテキストデータが入っている場合は出力\n",
    "        if( len(shapeText) >0 ):\n",
    "            print(shapeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "シート数 1\n",
      "['Sheet1']\n",
      "あ\n",
      "い\n",
      "う\n"
     ]
    }
   ],
   "source": [
    "book = xlrd.open_workbook('./input/エクセル.xlsx')\n",
    "\n",
    "# ブック内のシート数を取得\n",
    "num_of_worksheets = book.nsheets\n",
    "print(\"シート数\",num_of_worksheets)\n",
    "\n",
    "# 全シートの名前を取得\n",
    "sheet_names = book.sheet_names()\n",
    "print(sheet_names)\n",
    "\n",
    "#ブック内のシート数分繰り返し\n",
    "for iSheet in range(book.nsheets):\n",
    "    sheet = book.sheet_by_index(iSheet)\n",
    "    #行の数だけ繰り返し\n",
    "    for row_index in range(sheet.nrows):\n",
    "        #列の数だけ繰り返し\n",
    "        for col_index in range(sheet.ncols):\n",
    "            val = sheet.cell_value(rowx=row_index, colx=col_index)\n",
    "            #emptyとか入る場合もあるので、まずSTRに変換。\n",
    "            str_val = str(val)\n",
    "            if len(str_val) >0:\n",
    "                #print(str_val)\n",
    "                print(str_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n転移学習を用いたメラノーマ自動診断システムの構\\n築\\n\\n著者 吉田 拓也\\n著者別名 YOSHIDA Takuya\\nその他のタイトル AUTOMATED MELANOMA DIAGNOSIS SYSTEM WITH\\n\\nLEARNING USING DRRP CONVOLUTIONAL NEURAL\\nNETWORK\\n\\n発行年 2017-03-24\\n学位授与年月日 2017-03-24\\n学位名 修士(工学)\\n学位授与機関 法政大学 (Hosei University)\\nURL http://hdl.handle.net/10114/13736\\n\\n\\n\\n平成28年度\\n\\n修士論文\\n\\n転移学習を用いた\\nメラノーマ自動診断システム\\n\\n指導教員\\n\\n彌冨\\u3000仁\\n\\n法政大学大学院 理工学研究科 応用情報工学専攻\\n\\n15R4131 吉田\\u3000拓也\\n\\n\\n\\n目 次\\n\\n論文要旨 2\\n\\n第 1章 背景 3\\n\\n第 2章 関連研究 6\\n2.1 従来のメラノーマ自動診断に関する先行研究 . . . . . . . . . . . . . . . . . . 6\\n2.2 近年のメラノーマ自動診断システムの傾向 . . . . . . . . . . . . . . . . . . . 9\\n2.3 メラノーマ自動診断システムにおける本研究の位置付け . . . . . . . . . . . . 10\\n\\n第 3章 方法 11\\n3.1 データの前処理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n\\n3.1.1 体毛除去 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.1.2 長軸位置合わせ処理 . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n\\n3.2 Convolutional Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n\\n3.3 転移学習 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n\\n第 4章 評価実験 25\\n4.1 データセット . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4.2 実験内容 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n\\n4.2.1 Fine-tuningによる学習済みモデルの再学習 . . . . . . . . . . . . . . 26\\n4.2.2 CNNによるフルスクラッチ学習 . . . . . . . . . . . . . . . . . . . . . 28\\n\\n4.3 実験結果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n\\n第 5章 考察 31\\n5.1 実験結果の考察 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n5.2 今後の課題 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n\\n第 6章 まとめ 33\\n\\n謝辞 34\\n\\n参考文献 35\\n\\n業績 41\\n\\n1\\n\\n\\n\\n論文要旨\\n\\nメラノーマは悪性度の高い皮膚癌であり，見た目が母斑に酷似していることからその識別\\nは専門医でも難しく，熟練した皮膚科専門医でさえ正答率は 75 - 88%にとどまると言われて\\nいる．メラノーマは早期発見の場合は予後良好だが，発見が遅れると最悪死に至ることがあ\\nるため，早期発見を支援すべくメラノーマの自動診断に関する研究が進められてきた．従来\\nの自動診断システムは，I）腫瘍領域抽出，II）特徴量抽出・計算，III）機械学習による分類\\nの 3つのフェーズから構成されるが，様々なパターンの症例に対応できる汎用的な腫瘍領域\\n抽出手法，および特徴量設計は未だ確立されておらず，ゆえに扱える症例は限られていた．\\nそのため，近年ではCNN（Convolutional Neural Network）を用いたメラノーマ自動診断\\nシステムが提案され始めている．CNNではこれらの困難な処理を伴わずに，入力画像の生\\nの画素値から直接特徴量を学習の一環として獲得するため，扱える症例に制限がないという\\n利点がある．しかし一方で，CNNでは有効な特徴を学習するために膨大な学習データを要\\nするという問題があった．\\nそこで，本研究では，大規模データセットで事前に学習済みのモデルを用いたFine-tuning\\nによる転移学習を行うことで学習効率の向上を図った．また，学習で用いるデータセットに\\nは，一般的な data augmentationに加え，医学的知見に基づいた前処理を施すことで，少規\\n模データセットでも十分効果的な学習が行えるようにした．\\n本研究では診断が確定した 1,760例（メラノーマ 329例，母斑 1,431例）のダーモスコピー\\n像をデータセットとして用い，学習済みモデルを Fine-tuningにより再学習を行った．構築\\nされた分類器の性能評価は 5-fold cross-validationで行い，感度 84.8%，特異度 89.5%を得た．\\nまた，ROCカーブによる評価では，AUC: 0.873を達成した．\\n\\n2\\n\\n\\n\\n第1章 背景\\n\\nメラノーマは，悪性黒色腫という悪性度の高い皮膚癌の 1種であり，医学的には，皮膚の\\n色に関係するメラニン色素を生成するメラノサイトと呼ばれる皮膚細胞に悪性が認められる\\n病気とされている．過去数十年にわたり，メラノーマの発生率は世界的に増加傾向にあり，\\n日本も例外ではない．メラノーマの発生率はオーストラリアが最も高く，最新の統計による\\nと，人口 10万人あたりの発症者数は 53人に達している．さらに，2012年のメラノーマによ\\nる死亡者数は 1,617人に達したと報告されている [1]．メラノーマは皮膚癌の中でも特に進行\\n速度が速く，別の部位へ転移する可能性が非常に高い．メラノーマの進行度（臨床病期分類）\\nは，TNM分類を基に決定される [2]．Table. 1.1にメラノーマの臨床病期の診断基準を示す．\\n\\nTable. 1.1: メラノーマの臨床病期分類\\n病期 説明\\n0期 腫瘍が表皮内にとどまる\\nIA期 腫瘍の厚さが 1mm未満で，潰瘍形成 (細胞表層が破壊されると穴ができ，\\n\\n皮下層組織が透けてみえる)を伴わないものであり，腫瘍は表皮および真\\n皮の上層にみられる\\n\\nI期 IB期 腫瘍の厚さは 1mm未満であるが，潰瘍形成を伴うか，真皮または皮下層\\n下まで，または腫瘍の厚さが 1～2mmで潰瘍形成は伴わない\\n\\nIIA期 潰瘍形成を伴い，腫瘍の厚さが 1～2mmのもの，または潰瘍形成は伴わ\\nないが，腫瘍の厚さが 2～4mmのもの\\n\\nII期 IIB期 潰瘍形成を伴い，腫瘍の厚さが 2～4mmのもの，または潰瘍形成は伴わ\\nないが，腫瘍の厚さが 4mm以上のもの\\n\\nIIC期 腫瘍の厚さが 4mm以上で潰瘍形成を伴うもの\\nIII期 腫瘍の厚さ，潰瘍形成の有無にかかわらず，リンパ節や周辺の皮膚に転\\n\\n移がある\\nIV期 肺，肝臓，脳，骨，軟組織または消化管などの体内の他の臓器に転移が\\n\\nある\\n\\nメラノーマの 5年生存率は，0期や I期であれば 90%以上であり，II期でも 70 - 80%であ\\nるが，III期では 50%弱，IV期では 10%未満と予後は格段に悪くなる [3]．したがって，メラ\\nノーマによる死亡率を減少させるためには早期発見が最も重要である．しかし，Fig. 1.1に\\n示すように初期のメラノーマと母斑（すなわち良性の色素沈着した皮膚病変）は酷似してい\\nるため，肉眼による両者の区別はしばしば困難である．\\n\\n3\\n\\n\\n\\nFig. 1.1: メラノーマと母斑（良性腫瘍）の症例\\n\\nそのため，より詳しくメラノーマの皮膚表面の構造を観察する方法として，非侵襲的な表\\n皮造影技術であるダーモスコピー（dermoscopy）が導入された [4]. ダーモスコピーは，明\\nるい光源を使い 10 - 30倍程度に拡大し，無反射の条件を作って表皮内から真皮浅層までの\\n色素分布や血管構造を詳しく観察する手法であり，ダーモスコピーに用いる器具をダーモス\\nコープと呼ぶ．無反射にする仕組みは大きく分けて 2種類あり，1つはグラスプレートと皮\\n膚の間に超音波のジェルや鉱物油，アルコールなどを用いて皮膚表面を光学的に均一にする\\n方法であり，もう 1つは直交する偏光フィルタを光源の前と観察レンズの前に入れることに\\nより，物理的に表面反射を消す方法である [5]．\\nダーモスコピーの導入により，従来の巨視的な臨床画像と比較して表皮構造をより詳細に観\\n察できるようになったため，ABCD-rule [6]，Menzies’ scoring method [7]，7-point checklist\\n[8]，Modified ABC-point list [9]，3-point checklist [10]などダーモスコピーに基づくの数多\\nくの診断指標が提案された．なかでもABCD-ruleと 7-point checklistは皮膚科医の間で広く\\n認知されている診断指標である．ABCD-ruleはTable. 1.2に示す 8項目について評価し，そ\\nれぞれの重みをかけ合わせてTotal Dermoscopy Score（TDS）を計算する．TDS＜ 4.75で\\nあれば良性，4.75 ≤ TDS ≤ 5.45であれば悪性の疑いあり，5.45＜ TDSであれば悪性と診\\n断される．7-point checklistは Table. 1.3に示す 7項目の有無を評価し，それぞれの重みを\\nかけ合わせて 3ポイント以上であれば悪性と診断される．\\n医学を中心とする生命科学の文献情報を収集したオンラインデータベースであるMEDLINE\\nのエントリーを網羅した体系的な評価によると，ダーモスコピーの導入により臨床診断にお\\nける感度（正しくメラノーマと識別された割合）は 10 - 27%向上したと報告されている [11]．\\n\\n4\\n\\n\\n\\nTable. 1.2: ABCD-rule\\nCriterion Description Score Weight\\n\\nAsymmetry いくつの軸で非対称か 0-2 1.3\\nBorder 8方向の境界でいくつが明瞭か 0-8 0.1\\nColor 白・赤・薄茶・濃茶・青灰・黒のうち何色あるか 1-6 0.5\\nDifferential\\n\\nStructure\\n\\n網状の色素構造，枝状の色素構造，均一な構造，\\n点構造，粒状構造の 5構造のうち何種類がある\\nか\\n\\n0-5 0.5\\n\\nTable. 1.3: 7-point check list\\nCriterion Description Weight\\n\\nMajor criteria:\\n\\n1. Atypical pigment network 不規則な網構造 2\\n2. Blue-Whitish veil 青白い領域 2\\n3. Atypical vascular pattern 不規則な血管パターン 2\\nMinor criteria:\\n\\n4. Irregular streaks 不規則な枝状構造 1\\n5. Irregular pigmentation 不規則な色素沈着 1\\n6. Irregular dots/globules 不均一な点，粒状構造 1\\n7. Regression structures 色素抜け構造 1\\n\\n現在ダーモスコピーは皮膚科で普及しており，一般的に用いられる診断手法となっている．\\nしかし，ダーモスコピーによる診断はしばしば主観的なものであるため，正確な診断には豊\\n富な経験とトレーニングが要求される．ゆえに，診断経験の浅さは，診断の精度・再現性の\\n乏しさへつながる．さらに，熟練した皮膚科専門医によるダーモスコピー診断においても，\\nメラノーマの診断精度は 75 - 88%であると言われている [12–14]．\\nしたがって，視覚的解釈の困難さと主観性に起因する診断誤差を最小限にするためには，\\nコンピュータ化された画像解析技術の開発が必要不可欠である．このような背景から，人間\\nの主観によって左右されない客観的な診断を実現することを目的に，メラノーマの自動診断\\nシステムの開発が進められてきた．\\n\\n5\\n\\n\\n\\n第2章 関連研究\\n\\n本章では，メラノーマの自動診断に関する従来研究について概観したのち，従来手法が抱\\nえる問題点を挙げる．その後，メラノーマの自動診断に関する近年の研究動向について概観\\nしたのち，本研究の位置付けを述べる．\\n\\n2.1 従来のメラノーマ自動診断に関する先行研究\\nメラノーマの自動診断システムは，臨床現場で実際に用いられている診断指標（ABCD-rule\\nや 7-point checklistなど）と同様に病変部の形状や色調の情報に基づき，定量的に診断を行\\nう．従来のメラノーマ自動診断システムの全体の処理フローは，Fig. 2.1に示すように，大\\n別すると I）腫瘍領域抽出，II）特徴量設計・抽出，III）機械学習による分類 の 3つのフェー\\nズに分けられる．メラノーマの自動診断システムの開発では，これらすべてのフェーズが複\\n雑かつ診断精度に大きく影響を及ぼす重要な処理であるため，各フェーズで様々な手法が検\\n討されてきた．そしてこれまでに多くのグループが高い診断精度を報告する自動診断手法を\\n提案してきた [14–27]．メラノーマの自動診断に関する研究の体系的な総評は [28] に示され，\\nTable. 2.1には先行研究の概要を示す．Table. 2.1の研究で用いられている画像はすべてダー\\nモスコープにより撮影されたダーモスコピー像が用いられているが，それぞれデータセット\\nの構成（含まれる症例の種類）が異なるため，一概に結果の数値を比較・評価することがで\\nきないことに注意されたい．\\nCelebiら [23] は，腫瘍の辺縁部検出法の改善，およびRBF（Radial Basis Function）カー\\nネルを用いた SVM（Support Vector Machine）分類器により，476例の母斑と 88例のメラ\\nノーマをデータセットとして用いて，感度（SE：メラノーマの分類精度）93.8%，特異度（SP：\\n母斑の分類精度）92.3%を達成した．Liuら [26] は，腫瘍の対称性に焦点を当て，色素沈着\\n標高モデルとグローバルポイントシグネチャーを開発した．このモデルは対称性を計測する\\n4つの特徴だけを用いて，高い分類精度（感度 86.4%，特異度 82.1%）を達成した．Shimizu\\nら [27] は，メラノサイト病変だけでなく，非メラノサイト病変もサポートするタスク分解\\n戦略を用いた分類モデルを開発した．このモデルは，メラノーマ，母斑，基底細胞上皮腫\\n（BCC）および脂漏性角化症（SK）の分類を同時に行い，それぞれ 90.5%，82.5%，82.6%，\\n80.6%の分類精度を達成した．\\n上述した通り，自動診断で用いられる特徴量は，臨床診断と同じく病変の形状や色調など\\nの情報である．自動診断システムでは，これらの特徴量をダーモスコピー像から画像処理に\\n\\n6\\n\\n\\n\\nFig. 2.1: 従来のメラノーマ自動診断システムのパイプライン\\n\\nTable. 2.1: Overview of dermoscopy image classification studies\\n\\nSource Author Year Segmentation Classifier∗ Total # Mel.† Dys. ‡ SE SP comment ⋄\\nmethod images (%) (%) (%) (%)\\n\\n[15] Ganster et al. 2001 Thresholding+color clustering k-NN 5363 2 19 73 89\\n[16] Elbaum et al. 2001 Thresholding Linear 246 26 45 100 85\\n[17] Rubegni et al. 2002 Thresholding ANN 550 36 64 94.3 93.8\\n[18] Hoffmann et al. 2003 clustering+region growing ANN 2218 22 7 - - AUC=0.844\\n[14] Blum et al. 2004 - Logistic 837 10 11 82.3 86.9\\n[19] Oka et al. 2004 Thresholding Linear 247 24 76 87.0 93.1 Internet-based\\n[20] Burroni et al. 2005 Thresholding Linear 174 22⋆ 78 71.1 72.1 ⋆: only in situ cases.\\n[21] Seidenari et al. 2005 - Linear 459 21 17 87.5 85.7 AUC=0.933\\n[22] Menzies et al. 2005 Semi-auto+manual Logistic 2420 16 25 91 65\\n[23] Celebi et al. 2007 Region growing SVM 564 16 55 93.3 92.3 AUC=0.966\\n[24] Iyatomi et al. 2008 Thresholding+region growing ANN 1258 16 - 85.9 86.0 Internet-based ◃\\n\\nAUC=0.928\\n[25] Tenenhaus et al. 2010 Supervised KL-PLS 227 14 52 95 60 AUC=0.84\\n[26] Liu et al. 2012 - SVM 351 25 - 82.1 86.4 AUC=0.906\\n[27] Shimizu et al. 2014 Thresholding+region growing MS Linear 964 11 72 90.5 82.5 4 class AUC=0.856\\n\\n∗:k-NN:k-nearest neighbor, ANN: artificial neural network, Logistic: logistic regression, SVM: support vector machine, MS\\nLinear: multi-stratified linear models.\\nKL-PLS: Kernel logistic partial least square regression.\\n\\n†: Percentage of melanomas in the data set.\\n‡: Percentage of dysplastic nevi in the data set.\\n⋄: AUC: Area under the ROC curve.\\n◃: Available at http://dermoscopy.k.hosei.ac.jp\\n\\nより抽出する．しかし，臨床現場で撮影される症例の中には，Fig. 2.2に示す（a）-（d）の\\nような内的要因や（e）-（j）のような外的要因により特徴量の抽出が難しい症例が多く存\\n在する [29]．医師による診断であれば主観で行われるため，上記症例を例外的に対処するこ\\n\\n7\\n\\n\\n\\nとができるが，画像処理による機械的な特徴量抽出においてはこれら要因は大きな障害とな\\nる．ゆえに，実際にはこれらの症例はデータセットから取り除かれることが多く，従来の自\\n動診断システムで扱える症例には制限があった．さらに，最も重要な問題である，メラノー\\nマの識別に有効な画像特徴量の設計および抽出方法は未だ確立されておらず，未解決の問題\\nである．臨床所見（ABCD-ruleや 7-point checklist）の実施は，あくまで主観的な定義であ\\nり，医師間で幅広い定義が存在するため容易ではない．例えそれが診断精度の高い医師のも\\nのであっても，人間が定める診断基準には限界があり，分類精度は依然として制限される．\\n\\nFig. 2.2: 腫瘍領域抽出が困難な症例\\n\\n8\\n\\n\\n\\n2.2 近年のメラノーマ自動診断システムの傾向\\n近年，音声認識や画像認識のベンチマークテスト [30]で，高い識別精度を実現したことか\\nら，深層学習（Deep Learning）の有効性が広く認知されるようになった [31–34]．深層学習\\nとは，従来のニューラルネットワークよりも多層のニューラルネットワークを意味するが，一\\n口に深層学習と言っても，様々なアプローチがある．画像認識分野において主流でありベー\\nスとなるのがConvolutional Neural Network（CNN） [35–37] である．\\nCNNを用いたメラノーマ自動診断システムのパイプラインは従来システムのパイプライ\\nン（Fig. 2.1）に対して，Fig. 2.3のように置き換えられる．CNNによる特徴量抽出は学習\\nの一環として入力画像の生の画素値から直接行うため，従来システムのボトルネックとなっ\\nていた，「腫瘍領域抽出」，「特徴量設計」，「特徴量抽出」などの複雑な処理を伴わずに分類を\\n行うことができる．\\n\\nFig. 2.3: CNNを用いたメラノーマ自動診断システムのパイプライン\\n\\n深層学習が多くのコンペティションやタスクで著しい成果を挙げた影響から，既存の手法\\nの置き換えが急速に進んでいる．メラノーマ自動診断システムも例外ではなく，CNNを用\\nいたメラノーマ自動診断システムが既に考案されている [38–40]．\\nCodellaら [38] は，皮膚の構造と自然のシーンとの類推を強調し，CNNを自然画像で事前\\nに学習させた．メラノーマと非メラノーマ，メラノーマと非定型病変の 2種類の 2クラス分\\n類問題を実施し，それぞれ平均精度 93.1%，73.9%を達成した．これらの結果は，同じデータ\\n\\n9\\n\\n\\n\\nセットによって評価された従来のアプローチよりも優れていた．その後，Kawaharaら [39]\\nもまた，CNNに ImageNet [41] の自然画像で事前学習を行い，彼らは全結合層手前の最終\\nConvolution層までを特徴抽出器として用いて，10クラスのデータセットによる分類問題に\\nおいて 81.8%の分類精度を達成した．また，Estevaら [40] は，ごく最近，一般的なカメラ\\nとダーモスコープで撮影された画像で構成された 129,450例のデータセット（内ダーモスコ\\nピー像 3,374例）を用いて，学習済みの Inception-v3（GoogleNet）[42] に追加学習を行い，\\n性能を検証した．検証は携帯型顕微鏡で撮影された画像を用いた（1）ケラチノサイト病変\\nの分類（ケラチノーマ 65例，良性脂漏性角化症 70例），（2）メラノサイト病変の分類（メラ\\nノーマ 33例，良性母斑 97例），ダーモスコピー像を用いた（3）メラノサイト病変の分類（メ\\nラノーマ 71例，良性母斑 40例），という 3つのタスクで行われた．ROCカーブ（Receiver\\nOperating Characteristic curve）を用いた性能評価の結果，AUC（Area Under the Curve）\\nはそれぞれ 0.96，0.96，0.94を達成した．これは，同じ症例を 21人の皮膚科専門医により診\\n断した結果得られたAUCと，91%以上の領域で判定が一致した．これらの方法論では，い\\nずれも，大規模なドメインにより構築された学習済みモデルを知識転移させることで性能を\\n向上させている．この技術は転移学習 [43] と呼ばれ，あるドメイン（元ドメイン）の知識\\nを，解きたい問題のドメイン（目標ドメイン）の学習に利用する手法である．この手法はメ\\nラノーマのような学習データが少ない分類問題を扱うときに有効である．\\n\\n2.3 メラノーマ自動診断システムにおける本研究の位置付け\\nCNNのアーキテクチャはただ階層が深いだけでなく，多数の特徴を扱うため各層の幅が\\n広く，パラメータ数が数億から数百億と非常に大きい．そのため，CNNにおけるメラノー\\nマ分類問題は「腫瘍領域抽出」，「特徴量設計」，「特徴量抽出」などの複雑な処理は伴わない\\nが，その代わりに特徴を十分学習させるための大規模な学習データが必要となる．\\nしかし，メラノーマの症例画像は，医療機関などの限られた機関でしか入手できないため，\\n大量の学習データを用意し，学習を行うことは困難である．近年，メラノーマの分類コンペ\\nティション [44] が開催され，メラノーマのオープンデータが公開されるなど，以前と比べて\\n入手手段は増えたものの，依然として十分な症例数の確保は難しい．上記の先行研究では，\\n一般的なカメラで撮影された病変画像をデータセットに加えることで症例数の増加を図って\\nいるが，カメラにより撮影された画像はダーモスコピー像と比べ，拡大率，角度，照明など\\nの撮影環境のばらつきが生じてしまうため，実用的な症例は限られてくる．\\nそこで，本研究では，CNNにおける学習での有効な前処理の提案およびそれを用いた自\\n動診断システムの構築を行う．本システムでは，大規模データセットで学習済みのモデルの\\n各パラメータを初期値として用い，Fine-tuningによる転移学習を行う．転移学習による知\\n識転移と，医学的知見を考慮した前処理により，学習データ数の少なさを補う効率的な学習\\nの実現を期待する．\\n\\n10\\n\\n\\n\\n第3章 方法\\n\\n本研究では，深層学習を用いたメラノーマ／母斑の 2クラス分類問題を扱い，自動診断シ\\nステムを構築する．Fig. 3.1にシステムの全体の概略図を示す．提案手法は，学習フェーズ\\nと識別フェーズに大別される．以下より本研究で用いた技術について詳細を述べる．\\n\\nFig. 3.1: システムの概略図\\n\\n11\\n\\n\\n\\n3.1 データの前処理\\n医学的知見に基づくと，母斑の細胞は一定の速度でゆっくりと成長するために，その形状\\nは対称になる傾向がある．一方，メラノーマの癌細胞は無作為かつ旺盛に増殖するため，そ\\nの形状は非対称になる傾向がある．これらの傾向は，メラノーマの診断において臨床現場で\\n用いられているABCD-ruleの基準の 1つである ‘A’（Asymmetry of tumor）に表われてい\\nる．Fig. 3.2に腫瘍領域の長軸を水平方向に揃えた母斑およびメラノーマの症例をいくつか\\n示す．左から 3つずつの症例（（a）-（c）および（e）-（g））は典型的な母斑およびメラノー\\nマの症例であり，腫瘍領域の長軸を水平方向に揃えることでその形状特徴が顕著化される．\\nまた右の 2つの症例（（d）および（h））は例外であり，上記の傾向がみられない症例である．\\n後述するCNNでは，畳込み演算により，隣接する大きな濃淡値差や，特定方向の線分に\\n反応するような特徴量を自動的に生成するため，腫瘍領域の長軸位置合わせにより両者が持\\nつ形状特徴の差異を強調することで，識別に有効な特徴量の獲得が期待される.そのため，本\\n研究では，一般的な data augmentationに加え，以前我々が提案した腫瘍領域の長軸位置合\\nわせ処理 [45] を前処理として行った．以下より，長軸位置合わせ処理を行うための処理手順\\nについて詳細を述べる．\\n\\nFig. 3.2: 腫瘍領域の長軸を水平方向に位置合わせした母斑とメラノーマの症例\\n\\n12\\n\\n\\n\\n3.1.1 体毛除去\\n後述する長軸位置合わせ処理では，腫瘍領域特定フェーズにて色調情報を用いた閾値処理\\nを行う．この手法では，Fig. 2.2（g）-（j）に示すようなアーティファクトは排除できるが，\\n体毛が腫瘍領域にオーバーラップしている症例においては腫瘍領域の特定が難しい．そのた\\nめ，事前にダーモスコピー像に映り込む体毛を除去する必要がある．ここでの処理は大別し\\nて I）体毛抽出，II）画像修復 の 2つのフェーズに分けられる．以下に各フェーズでの処理\\nの詳細を述べる．\\n\\nI）体毛抽出：\\u3000体毛抽出の手法として，本研究では，FrangiおよびSatoらが提案したヘッ\\nセ行列に基づく線状構造抽出を行うmulti-scaleフィルタ [46,47] を用いる．multi-scale\\nフィルタは，入力画像に二次元ガウシアンフィルタを適用し，雑音除去を行うと同時\\nに二次微分によって体毛のような線状構造を強調するフィルタである．二次元画像の\\n濃淡値関数 I(x) (x = (x, y))における注目点x0近傍の濃淡値分布は，テイラー展開に\\nより以下の二次元連続関数で近似できる．\\n\\nI(x) = I(x0) + (x− x0)∇I(x0) +\\n(\\n1\\n\\n2\\n\\n)\\n(x− x0)∇2I(x0)(x− x0) (3.1)\\n\\nここで，x0における 2× 2ヘッセ行列は，以下のように表現できる．\\n\\n∇2I(x) =\\n\\n⎡\\n\\n⎣I(x)xx I(x)xy\\nI(x)yx I(x)yy\\n\\n⎤\\n\\n⎦ (3.2)\\n\\nただし，I(x)xxなどは I(x)の各軸方向における濃淡値の二次偏導関数で，Ixiyj(x) =\\n∂2\\n\\n∂xi∂yj\\n，i, jは i+ j = 2を満たす非負の整数とする．体毛抽出処理では，まず画像の各\\n\\n画素におけるヘッセ行列を求める．I(x)のヘッセ行列の計算は，Eq. 3.3のようにガウ\\nス関数と画像の畳込みによって求める．\\n\\nIxiyj(x) =\\n∂2\\n\\n∂xi∂yj\\nG(x, σ) ∗ I(x) (3.3)\\n\\nここで，σはガウス関数の標準偏差を表す．このガウス関数の標準偏差 σの値を調節\\nすることにより，強調される線の幅を変えることができる．次に畳込みによって得ら\\nれたヘッセ行列に対して，固有値と固有ベクトルを求める．ヘッセ行列∇2I(x)の固有\\n値を λ1,λ2 (λ1 > λ2)，固有ベクトルを e1，e2と表す．一般に線状構造では，線の断面\\n方向への濃淡変化が大きく，線の進行方向への濃淡変化は小さい．したがって，ヘッ\\nセ行列の固有値の関係から線状性を調べることができ，理想的な線状構造の中心点で\\nは，λ2が最小となり，λ1 = 0となる．すなわち，線状構造を強調する場合の固有値の\\n関係式は，以下の Eq. 3.4のようになる．\\n\\n13\\n\\n\\n\\nλ2 << λ1 ≈ 0 (3.4)\\n\\nヘッセ行列の固有値 λ1,λ2の値がこの関係式に近いほど，画素の線状構造が強いこと\\nを表す．各画素の線状性をEq. 3.5で定義する．\\n\\nλ12 =\\n\\n⎧\\n⎪⎪⎪⎪⎨\\n\\n⎪⎪⎪⎪⎩\\n\\n|λ2|\\n(\\n1 + λ1|λ2|\\n\\n)\\n= |λ2|+ λ1 (λ2 ≤ λ1 ≤ 0)\\n\\n|λ2|\\n(\\n1− α λ1|λ2|\\n\\n)\\n= |λ2|− αλ1 (λ2 < λ1 <\\n\\n|λ2|\\nα\\n)\\n\\n0 (otherwise)\\n\\n(3.5)\\n\\nここで，αの値は，0 ≤ α ≤ 1で，線の双極性に対する許容度を表す．この λ12の値が\\n大きいほど，線状構造に近いことを表す．multi-scaleフィルタによる体毛抽出の結果\\nを Fig. 3.3（b）に示す．\\n\\nII）画像修復：\\u3000前フェーズで抽出した体毛をマスクとして用いて，マスクにより選択さ\\nれた体毛領域の除去および除去部の修復を行う．画像修復の手法には，Bertalmioらが\\n提案した Image Inpainting [48] のアルゴリズムを用いた．Image Inpaintingは濃淡値\\nの連続性を用いた手法であり，欠損領域の周りから濃淡値を滑らかに補間することで\\n欠損領域を修復する．そのため，体毛のような線状構造領域の修復において有効に働\\nく．Image Inpaintingのアルゴリズムは，修復したい領域 P の画素に対して，次の反\\n復処理を行うことによって与えられる．\\n\\nIn+1(x, y) = In(x, y) +∆Int (x, y), ∀(x, y) ∈ P (3.6)\\n\\nここで，添字の nはEq. 3.6の反復回数を，(x, y)は修正する画素の座標位置を，Int は\\n画像 Inへの更新値をそれぞれ表している．以上の反復処理は，In+1(x, y) = In(x, y)（\\n∀(x, y) ∈ P）が成立した場合に終了する．画像の修復結果の良し悪しは，更新値 Int の\\n与え方によって左右される．Bertalmiaらは，更新値 Int の定義として Eq. 3.7を与え\\nている．\\n\\nInt (x, y) = ∇L\\nn(x, y) ·Nn(x, y) (3.7)\\n\\nここで，LnとNnは次式で定義される．\\n\\nLn(x, y) = Inxx(x, y) + I\\nn\\nyy(x, y), N\\n\\nn(x, y) = ∇In(x, y)⊥ (3.8)\\n\\n以上のアルゴリズムに基づく画像修復の結果を Fig. 3.3 (c)に示す．\\n\\n14\\n\\n\\n\\nFig. 3.3: 腫瘍領域にオーバーラップしている体毛の除去処理\\n\\n15\\n\\n\\n\\n3.1.2 長軸位置合わせ処理\\nFig. 3.4に長軸位置合わせ処理のフローを示す．長軸位置合わせ処理は I）初期領域特定，\\n\\nII）領域選択，III）長軸の計算・位置合わせ の 3つのフェーズから構成される．この長軸位\\n置合わせ処理では腫瘍領域の長軸を特定するために腫瘍領域の抽出処理を伴うが，辺縁部の\\n詳細な情報は必要としないため精密さは問わない．以下より各フェーズの処理の詳細を示す．\\n\\nFig. 3.4: 腫瘍領域の長軸位置合わせの処理フロー\\n\\n16\\n\\n\\n\\nI）初期領域特定：\\u3000まず入力されたダーモスコピー像に対して，ヒストグラム均等化法\\nを用いたコントラスト強調を施し，画像中の濃淡値分布の偏りをなくす．この処理は\\nFig. 2.2（a）のような，腫瘍領域とその周囲の肌の色のコントラストが低い症例にお\\nいて，腫瘍領域の特定の際に有効に働く．本研究では，Iyatomiら [24] の提案手法に\\n基づき，色調情報を用いた閾値処理により腫瘍領域の特定を行う．この手法では，ま\\nずダーモスコピー像にガウシアンフィルタを用いた雑音除去を行う．その後，ラプラ\\nシアンフィルタを用いて濃淡変化の大きい画素値の座標集合を取得した．集合に含ま\\nれる画素に対応する元の画素の B（青）の成分を用いて，大津の閾値判別法により閾\\n値を決定し，暗い部分を初期腫瘍領域とした．この手法は色調情報に基づくものであ\\nるため，Fig. 2.2に示す（g），（h），（i），（j）などのアーティファクトの有無に関わら\\nず腫瘍領域の特定が可能である．\\n\\nII）領域選択：\\u3000前フェーズの閾値処理により，無数の独立した小領域が生成されるため，\\nこのフェーズでは最初にそれぞれをある程度の大きさを持つ領域にまとめる処理を行\\nう．すべての小領域に対してラベリング処理を行い，各領域において，画像全体の 1%よ\\nりも小さい領域は，自領域よりも大きく最も長い境界線を共有する領域に統合する．こ\\nの処理をすべての領域に対して領域数が十分小さくなるまで繰り返し行った上で，な\\nお残った腫瘍領域候補に対して，次は腫瘍領域とそれ以外の領域とを区別する処理を\\n行う．この処理は，Fig. 3.5に示す処理フローに基づいて行う．この処理フローは，i）\\nで独立した小領域を取り除き，ii）で濃淡値が低く閾値処理で抽出できなかった腫瘍領\\n域を見つけ出し，iii）および iv）でFig. 2.2（e）のようなダーモスコピー像に映り込ん\\nだ黒枠を取り除く．ここで，Fig. 3.5におけるRとは，画像の端に触れる最も長い領\\n域を表す．\\n\\nFig. 3.5: 領域選択の処理フロー\\n\\n17\\n\\n\\n\\nIII）長軸の計算・位置合わせ：\\u3000このフェーズでは，まず腫瘍領域の重心を算出する．重\\n心は幾何学的に，ある図形の，その周りでの一次モーメントが 0であるような点を重\\n心と定義した．腫瘍領域 P の各点 rが密度 f(r)を持つとき，その重心 gは，\\n\\n∫\\n\\nP\\n\\n(g − r)f(r)dV = 0 (3.9)\\n\\nを満たす点である．本研究では，腫瘍領域の輪郭上の 2点を結ぶ線分が，この重心付近\\nを通過し，かつ最長となるとき，その線分を腫瘍領域の長軸として定義した．長軸が\\n決定したら，この長軸が水平方向に沿うように回転させ位置合わせを行う．後述する\\nCNNでは正方行列の画像を入力に取るため，位置合わせ後は腫瘍領域を中心に正方行\\n列となるようトリミングを行う．以上の長軸位置合わせ処理は，Fig. 3.1のシステム全\\n体の処理フローに示す通り，学習用，評価用問わず，すべての画像に対して行われる．\\n\\n18\\n\\n\\n\\n3.2 Convolutional Neural Network\\n\\nConvolutional Neural Network (CNN)は，複数のConvolution層，Pooling層，LCN (Local\\nContrast Normalization) 層と全結合層から構成される順伝播型ネットワークである．学習は\\n誤差逆伝播法（Back-Propagation）と確率的勾配降下法（Stochastic Gradient Descent）に\\nより最適化を行う．\\nConvolution層では，入力画像に対し，複数個のフィルタを並行して畳込み演算を行う．画\\n像の縦横の画素数をW ×W，チャネル数をKとおき，画像サイズをW ×W ×Kと表す．\\n畳込み演算の結果得られる中間層での出力は特徴マップと呼ばれる．Fig. 3.6を用いて畳込\\nみ層が行う計算処理を述べる．\\n\\nFig. 3.6: Convolution層の処理\\n\\nこのConvolution層は第 l層に位置し，l−1層からKチャネルの画像 z(l−1)ijk (k = 0, ..., K−1)\\nを受け取り，これにM (= 3)種類のフィルタ hpqkm (m = 0, ...,m− 1)を適用している．各\\nフィルタは入力と同じチャネル数Kを持ち，そのサイズをH×H×Kとする．Fig. 3.6のよ\\nうにm = 0, 1, 2の各フィルタm (= 0, 1, 2)について並行に計算が実行され，それぞれ 1チャ\\nネルの uijmが出力される．その計算は，各チャネル k (= 0, ..., K − 1)について並行に画像\\nとフィルタの畳込みを行った後，結果を画素ごとに全チャネルにわたって加算するもので，\\n\\n19\\n\\n\\n\\nuijm =\\nK−1∑\\n\\nk=0\\n\\nH−1∑\\n\\np=0\\n\\nH−1∑\\n\\nq=0\\n\\nz\\n(l−1)\\ni+p,j+q,k + hpqkm + bijm (3.10)\\n\\nのように表される．このように，入力画像のチャネル数によらず，1つのフィルタからの出力\\nは常に 1チャネルになる．また，bijmはフィルタごとのバイアスを表し，その値は各ユニッ\\nト共通（bijm = bm）である．畳込み演算の結果 uijmそのものは単純な線形射影であるため，\\nその出力に適切な活性化関数を用いて非線形性を与える必要がある．一般的にここでの活性\\n化関数には，ReLU（Rectified Linear Units）関数が用いられることが多い．この関数はEq.\\n3.11で表され，Fig. 3.7に示すように，入力の値の大きさによって勾配は減少せず，正なら\\nどこでも一定の値をとる．ReLU関数 f(x)の出力 zijmは Eq. 3.12で表される．\\n\\nFig. 3.7: ReLU関数\\n\\nf(x) = max(0, x) (3.11)\\n\\nzijm = f(uijm) (3.12)\\n\\nFig. 3.6のように，この値がConvolution層の最終的な出力となり，その後の層へと伝わる．\\nConvolution層の出力を入力として扱う Pooling層では，入力された特徴マップ上に局所\\n領域をとり，領域内の値を統計し集約する処理を行う．サイズW ×W ×Kの入力画像上で，\\n画素 (i, j)を中心とするH ×H正方領域をとり，この中に含まれる画素の集合をPijで表す．\\nこのPij内の画素について，チャネルKごとに独立に，H2個ある画素値を用いて 1つの画素\\n値 uijkを求める．正方領域内の画素値の決め方はいろいろあるが，今回は領域内の最大値を\\n出力とするmax poolingを用いた．max poolingの概要および計算を，以下の Fig. 3.7，Eq.\\n3.13にそれぞれ示す．\\n\\nuijk = max\\n(p,q)∈Pij\\n\\nzpqk (3.13)\\n\\nこのように入力情報の一部を切り捨てることで，各層間の結合（重み）に過疎生を持たせ\\nることができ，入力画像の位置ずれなどの幾何学的変化に対して不変性が得られる．この処\\n理により上層へいくほど広い受容野の情報が抽象化されるため，ゆえに不変性は層を経るご\\nとに強くなる．\\n\\n20\\n\\n\\n\\nFig. 3.8: Pooling層の処理（max pooling）\\n\\nCNNにおける正規化は，入力画像を対象に行うものと，CNNの途中経過を対象に行うも\\nのの 2つある．前者は，学習画像全体の画素ごとの平均を求め，対象とする画像からこの平\\n均を差し引き，正規化するものである．それぞれの計算式をEq. 3.14，Eq. 3.15に示す．た\\nだし，x(n)ijkは n番目のサンプルの画素 (i, j)のチャネル kの値を表す．\\n\\nx̄ijk =\\nN∑\\n\\nn=1\\n\\nx\\n(n)\\nijk (3.14)\\n\\nxijk = xijk − x̄ijk (3.15)\\n\\n一方後者は，画像 1枚 1枚に対し個別に行う処理であり，LCN層が担っている．LCN層\\nでは全チャネルにわたる局所領域 Pij ごとに画素の平均 x̂ij を求めて減算，そこから標準偏\\n差 σijをとり，除算を行う．Kチャネルからなる画像 xijkを対象とするとき，重み付き平均\\nx̂ijkは Eq. 3.16で求まる．\\n\\nx̂ij =\\n1\\n\\nK\\n\\nK−1∑\\n\\nk=0\\n\\nwpqxi+p,j+q,k (3.16)\\n\\nここで，重みwpqはガウシアン関数であり，\\n∑\\n\\npqk wpq = 1となる．減算正規化は，画素 (i, j)\\nごとに違うが，チャネル間では共通の x̂ijを差し引いて，Eq. 3.17のように行われる．\\n\\nzijk = xijk − x̂ij (3.17)\\n\\n21\\n\\n\\n\\n画像の全チャネルにわたる局所領域 Pijの分散は Eq. 3.18のように計算される．\\n\\nσ2ij =\\n1\\n\\nK\\n\\nK−1∑\\n\\nk=0\\n\\n∑\\n\\n(p,q)∈Pij\\n\\nwpqk(xi+p,j+q,k − x̂ij)\\n2\\n\\n(3.18)\\n\\nこの分散 σ2ijを用いて，除算正規化は Eq. 3.19のように計算される．\\n\\nzijk =\\nxijk − x̂ij√\\n\\nc+ σ2ij\\n(3.19)\\n\\nコントラスト正規化では，このように局所領域内の全チャネルの画素を対象に平均と分散を\\n計算する．この操作により，画像上の局所領域内の各画素を平均 0，分散 1になるように濃\\n淡が正規化される．これには重みの値が大きな値にならないように調整し，過学習を抑制す\\nる効果がある．LCN層は通例として Pooling層の次に配置されることが多い．\\n全結合層はここまでの特徴抽出部から最終的に出力された特徴マップを入力にとり，各層\\n間の重みは全結合される．前層からの入力は一次元ベクトルとしてみなされ，全結合層の各\\nユニットはEq. 3.20，Eq. 3.21のように線形の和で計算される．全結合層の出力ユニット数\\nnは分類するクラス数である．\\n\\nhj = wj1x1 + wj2x2 + · · ·+ wjixi + bj (3.20)\\n\\nhj = f(w\\nTx+ bj) (3.21)\\n\\n本研究では最終出力の活性化関数に，Eq. 3.22に示すソフトマックス関数（softmax function）\\nを用いて，算出した全結合層のユニット値に対してクラスの分類を行った．ソフトマックス\\n関数は出力層の各ユニット値の和（出力の合計）が\\n\\n∑n\\nj yi = 1となるため，出力値は各クラ\\n\\nスに対する確率分布を表す．ゆえにソフトマックス関数で算出された値の中で，yiが最大値\\nをとるユニットを推定クラスとする．\\n\\nyi =\\nexp(hi)∑n\\nj exp(hj)\\n\\n(3.22)\\n\\n冒頭で述べた通り，CNNの学習は確率的勾配降下法による誤差逆伝播法で行われ，ネッ\\nトワークの出力と教師データの誤差を最小にするように各種パラメータ（畳込み層のフィル\\nタの係数，ユニットのバイアス，全結合層の結合荷重とバイアス）の更新が行われる．本研\\n究ではミニバッチ学習（mini-batch learning）を取り入れた確率的勾配降下法を用いた．こ\\nの学習では，学習データ nをほぼ等しいサイズの k個のデータ群（m1,m2, · · · ,mk）に分割\\nし，m1からmkまでの各データ群を逐次的に入力としてとり，1つのデータ群が渡される度\\nにパラメータの更新を行う．また，誤差関数にはEq. 3.23に示す交差エントロピー誤差関数\\n（Cross-entropy error function）を用いた．\\n\\n22\\n\\n\\n\\nE(w) = −\\n1\\n\\nN\\n\\nN∑\\n\\nn=1\\n\\n{tn ln(y(xn, w) + (1− tn) ln(1− y(xn, w))} (3.23)\\n\\nこの式は入力データに対する教師信号 t1, · · · , tnと識別結果 y1, · · · , ynの乖離を表している．\\nこの式をミニバッチ学習に対応させると\\n\\nE(w) =\\nN∑\\n\\ni=1\\n\\nEi(w) (3.24)\\n\\nと書き換えることができ，En(w)は各データ群についての誤差を表す．このとき確率的勾配\\n降下法の更新式は\\n\\nw(τ+1) = w(τ) − ηi∇Ei(w(τ)) (3.25)\\n\\nとなる．ただし τ は繰り返しの回数であり，ηは学習率パラメータである．また，本研究で\\nは学習の高速化，および汎化能力の向上を図るため，上記の更新式に対して 2つの重みの正\\n規化手法を行った．1つ目はMomentum法と呼ばれる手法であり，更新式の前回の重みの項\\nに対してモーメント係数を付けることで，重みの振動を抑える．重みの変化が前回の学習と\\n同じ方向ならば大きく，違う方向ならば小さく変化させる程度を表し，モーメント係数を µ\\nとすると更新式は以下になる．\\n\\nw(τ+1) = µw(τ) − ηi∇Ei(w(τ)) (3.26)\\n\\n2つ目はWeight Decay法と呼ばれる手法であり，この手法では不要な結合荷重が大きく\\nなり過ぎないようなペナルティ項を加えることにより，結合荷重の絶対値が小さくなる方向\\nに働き，学習が進むにつれて不要な結合荷重が 0に近付くようになる．Weight Decay係数\\nを λとすると更新式は以下になる．\\n\\nw(τ+1) = µw(τ) − ηi∇Ei(w(τ))− ηλw(τ+1) (3.27)\\n\\nこの更新式を基に，順伝搬（現パラメータ群により各学習データの認識を行う）と逆伝搬\\n（分類結果をもとにパラメータ群を更新する）を繰り返し行うことで学習の収束を図る．\\n\\n23\\n\\n\\n\\n3.3 転移学習\\n転移学習（Transfer learning）とは，あるドメイン（元ドメイン）の知識を解きたい問題\\nのドメイン（目標ドメイン）の学習に利用する手法である．学習データが非常に少ない，あ\\nるいは全く存在しない場合でも，元ドメインの知識を利用して分類器を学習できるという特\\n性がある．一般に，Fig. 3.9に示すように学習済みモデルを目標ドメインの特徴抽出器とし\\nて用いるアプローチと，Fig. 3.10に示すように目標ドメインのデータセットを用いて再学習\\nを行うアプローチがある．後者は特に Fine-tuningと呼ばれ，学習済みモデルの識別層，あ\\nるいはその 1つ前のConvolution層だけを目標ドメインに置き換えて，誤差逆伝播法による\\n再学習を行う手法である．その他の部分は学習済みモデルのパラメータをそのまま初期値と\\nして用いることが多い．CNNの学習は初期値依存性が強く，特に学習データが少ない場合は\\nできるだけよい初期値を得ることが，過学習を防ぎよい学習結果を得るために重要である．\\nそのため，学習済みモデルを適切に選択し，初期値として用いた転移学習を行うことで，フ\\nルスクラッチから学習するよりも効率的に学習を進められる場合が多い．\\n\\nFig. 3.9: 転移学習：学習済みモデルを目標ドメインの特徴抽出器として用いる場合\\n\\nFig. 3.10: 転移学習：目標ドメインのデータセットを用いて Fine-tuningを行う場合\\n\\n24\\n\\n\\n\\n第4章 評価実験\\n\\n本章では，実験で用いたデータセットや手法などの詳細を述べる．また，実験により得ら\\nれた分類器に対して性能評価を行った結果を示す．\\n\\n4.1 データセット\\n本研究では，2つの医療機関（University Federico II of Naples, Italy; University of Graz,\\n\\nAustria）およびオープンデータセット（International Skin Imaging Collaboration (ISBI) [44]）\\nから入手したダーモスコピー像で構成される 1,760枚（メラノーマ：329枚，母斑：1,431枚）\\nをデータセットとして用いた．ここで，3つの機関から入手した病変のラベルは，すべて生\\n検の病理組織学的検査に基づいて診断が確定されたものである．これらすべての症例は前処\\n理として，前述した長軸の位置合わせ処理に加え，一般的な data augmentationを行った．\\n実施した data augmentationは順に，I）垂直方向．水平方向への反転処理，II）ガウス雑音\\nの付加 の 3つである．ガウス雑音は各画素の輝度値を中心に標準偏差 0 - 50の範囲でランダ\\nムに 5段階の雑音を付加する．以上の data augmentationにより，元のデータセットを 20倍\\nの 35,200枚（= 1, 760 × 4 × 5）まで拡張した．また，各機関の画像は，画像サイズにばら\\nつきがあるが，長軸の位置合わせ処理の段階で腫瘍領域を中心に正方行列にトリミング，お\\nよび 256× 256 pixelにリサイズした．\\n以上のデータセットは，Fig. 1.1に示すような良好な撮影条件で撮られた症例の他に，Fig.\\n\\n2.2に示すようなアーティファクトが映り込んだ症例も含まれる．\\n\\n25\\n\\n\\n\\n4.2 実験内容\\n本実験は，提案手法に基づき，学習済みモデルのパラメータを初期値として用いて，そこ\\nから Fine-tuningを行い，メラノーマ／母斑の 2クラス分類器を構築した．また，比較実験\\nとして，小規模なCNNを用いた同じデータセットによるフルスクラッチでの学習を行った．\\n各実験の詳細を以下に述べる．\\n\\n4.2.1 Fine-tuningによる学習済みモデルの再学習\\n本実験では，学習済みモデルに，2014年に開催された大規模画像認識コンペティション\\n\\n（ILSVRC）[30]で 2位の成績を収めたVGG-19 [34]を用いた．VGG-19は，Fig. 4.1左図に示\\nすように，Convolution層とPooling層から構成されるシンプルなCNNである．重みのある\\n層（Convolution層，全結合層）を 19層持つ深いネットワークであり，3×3の小さなフィルタ\\nを用いた畳込みを連続して行っているのが特徴である．実験で使用したVGG-19は ILSVRC\\n2014にて構築されたものと同じく ImageNet [41]を用いて学習されている．ImageNetの1,000\\nクラスには，メラノーマおよび母斑のクラスは含まれていないため，元ドメインと目標ドメ\\nインのカバーする領域は異なることが想定される．そのため，今回の実験ではVGG-19を単\\nに特徴抽出器として用いるのではなく，Fig. 4.1右図に示すように，学習済みのVGG-19の\\n各パラメータを初期値として用いて，VGG-19の全結合層手前のConvolution層以降のパラ\\nメータをFine-tuningすることで分類器の構築を行った．また，CNNでは浅い層ほどエッジ\\nやブロブなど汎用的な特徴が抽出されるのに対し，深い層ほど学習データに特化した特徴が\\n抽出される傾向を考慮して，最終 Convolution層より前の浅い層のパラメータに関しては，\\n初期パラメータのまま固定し，深い層の重みのみを，メラノーマ／母斑の目標ドメインに合\\nうように再調整した．\\n本実験は，Caffeフレームワーク [49] を用いて行った．学習済みのモデルをメラノーマ／\\n母斑のデータで再学習するため，最後の全結合層の出力を 1,000クラスから 2クラスに変更\\nし，全体の学習率は 0.001と低く設定した．ただし，Fine-tunigを行う最終 Convolution層\\nと全結合層は新規にメラノーマ／母斑の 2クラスを学習するため，重みとバイアスの学習率\\nの乗数は高い値に設定した．\\n\\n26\\n\\n\\n\\nFig. 4.1: 学習済みVGG-19モデルを用いた Fine-tuning\\n\\n27\\n\\n\\n\\n4.2.2 CNNによるフルスクラッチ学習\\n本研究では，比較実験のため事前学習なしのCNNにおいても同じデータセットを用いて\\n実験を行った．本実験ではメラノーマ 329枚，母斑 1,431枚と比較的小規模なデータセットを\\n扱うため，CNNの構成は Fig. 4.2に示すように，Convolution層と Pooling層が 3層連なっ\\nた小規模なモデル構成とした．具体的な構成として，まず第 1層目の畳込み層は 256 × 256\\nの入力画像がランダムクロップされた 224× 224× 3を入力として取り，サイズ 11× 11× 3\\nの 96個のフィルタを用いて畳込みを行う．第 2層目の畳込み層は，第 1層目のレイヤの出力\\nを入力として取り，サイズ 7 × 7 × 96の 48個のフィルタを用いて畳込む．そして第 3層で\\nは，第 2層目のレイヤの出力を入力として取り，サイズ 5× 5× 48の 128個のフィルタを用\\nいて畳込む．第 3層目のレイヤの出力は全結合層へと渡される．各レイヤの畳込みの出力に\\n対しては，サイズ 3 × 3のフィルタを用いたmax pooling，および局所コントラスト正規化\\nを行った．また，長軸位置合わせ処理の効果を確認するため，長軸位置合わせ処理を施して\\nいない同症例のデータセットに関しても実験を行った．\\n\\nFig. 4.2: CNNによるフルスクラッチでの学習\\n\\n28\\n\\n\\n\\n4.3 実験結果\\n学習は，バッチサイズ 100のミニバッチ学習に基づく確率的勾配降下法で行い，損失関数\\nに交差エントロピー誤差関数を用いた．学習回数は 100 epochであり，各ミニバッチ処理の\\n終了時に重み，およびバイアスが更新される．両分類器共，100 epochの学習で十分誤差は\\n減少し，収束が確認された．分類器の性能評価は 5-fold cross-validationを用いて行い，各分\\n類器の感度（SE: Sensitivity），特異度（SP: Specificity），およびAUC（ROCカーブの下面\\n積）を算出した．ここで，感度と特異度は，それぞれ以下のEq. 4.1，Eq. 4.2で定義される．\\n\\n感度 = 悪性と分類された症例数全体の悪性の症例数 × 100 (%) (4.1)\\n\\n特異度 = 良性と分類された症例数全体の良性の症例数 × 100 (%) (4.2)\\n\\n各分類器の感度，特異度，およびAUC（ROCカーブの下面積）に関して，実験から得られ\\nた結果を，以下のTable. 4.1に示す．\\n\\nTable. 4.1: メラノーマ／母斑の分類性能評価\\nSE (%) SP (%) AUC\\n\\nFine-tuning CNN 84.8 89.5 0.873\\n\\nFull-scratch CNN 80.9 86.1 0.847\\n\\nFull-scratch CNN without alignment method 74.9 80.9 0.789\\n\\n29\\n\\n\\n\\n以下の Fig. 4.3に，分類結果から得られたROCカーブを示す．\\n\\nFig. 4.3: ROCカーブ\\n\\n30\\n\\n\\n\\n第5章 考察\\n\\n5.1 実験結果の考察\\nまずフルスクラッチ学習のCNNにおける長軸位置合わせ処理の有無の比較結果から，長\\n軸位置合わせ処理をデータセットに施すことにより精度が大きく向上することが確認できた．\\nそして提案手法である Fine-tuningによる転移学習の実施は，ROCカーブにおけるAUCに\\nよる評価で，2.6%の分類性能を改善した．フルスクラッチで学習を行ったCNNにおいても，\\n最終的に示した分類性能は決して悪くはなかったものの，Fine-tuningによる分類精度には\\n劣った．さらに，フルスクラッチでの学習は，Fine-tuningによる学習に対し，学習誤差の\\n収束が遅かったという懸念がある．このことから，転移学習では十分学習能力が備わったモ\\nデルのパラメータを初期値として用いたことにより，効率的な学習ができていることを確認\\nした．転移学習は一般的に事前学習に用いる元ドメインのデータが目標ドメインを内包する\\nものでなければ効果は薄いとされるが，メラノーマ／母斑クラスにも適用できたことは非常\\nに興味深い結果である．また，一般的に Fine-tuningはある程度の規模のデータセットによ\\nり行わなければ，十分な知識転移は行えないとされるが，結果では 35,200枚の小規模なデー\\nタセットで，十分に高性能な分類器の構築を行うことができた．少量の学習データにも関わ\\nらず汎用性の高い分類器を構築できたのは，前処理として行った腫瘍領域の長軸の位置合わ\\nせ処理を実施したことにより，メラノーマや母斑が持つ形状特徴の差異を強調することがで\\nき，学習効果の高いデータへ変換することができたことに起因すると考えられる．これによ\\nり，メラノーマと母斑を分類するのに適切な特徴量の獲得が行われたのではないかと考える．\\nこの前処理の有効性の検討は，以前我々が行った検証の結果より，CNNにおける分類にお\\nいて効果的であることが実証されている．以上の結果から，適切な知識転移を行うことで，\\n元ドメインがカバーしない領域の目標ドメインにおいても十分な分類性能を示せるという知\\n見を得ることができた．\\n\\n31\\n\\n\\n\\n5.2 今後の課題\\nメラノーマの自動診断システムには，診断結果に至った根拠を利用者に提示するという課\\n題がまだ残されている．本研究で構築したシステムは入力された症例画像に対してメラノー\\nマ／母斑を分類した結果のみの提示であったが，医療診断システムにおいては信頼性こそが\\n最も重要であるため，根拠に基づいた診断結果をシステム利用者にわかりやすい形で提示す\\nる必要がある．これに対して現在，構築された本システムを特徴抽出器として利用し，ダー\\nモスコピー診断経験のある皮膚科専門医 4名に依頼したABCD-rule（8項目）および 7-point\\nchecklist（7項目）で評価済みのデータを入力とし，ベクトル化されたデータを用いて，上\\n記の各項目 (計 15個)＋診断結果 (メラノーマ／母斑)の 16項目に対して各推定項目の予測\\n誤差の評価を行っている．しかしながら，診断結果と上記診断指標で整合性が取れていない\\nのが現状である．本研究の背景でも述べたが，これらの診断指標はあくまで主観的な定義で\\nあるため，画像特徴量による診断指標の数値回帰は容易なタスクではなく，研究の余地が大\\nいにある．\\n\\n32\\n\\n\\n\\n第6章 まとめ\\n\\n本研究では，転移学習技術を用いた学習済みモデルのFine-tuning，および医学的知見に基\\nづいたCNN学習における効果的な前処理の実施により，感度 84.8%，特異度 89.5%，AUC\\n0.873 の高性能な分類器を構築することができた．この数値は熟練した皮膚科専門医に匹敵\\nするものであり，従来の自動診断システムで実施されてきた「腫瘍領域抽出」，「特徴量設計」，\\n「特徴量抽出」などの複雑な処理を伴わずとも，十分な分類性能を達成した．これは，従来\\nの自動診断システムでは扱うことができなかった症例においても診断が可能になったことを\\n意味する．今後は診断結果だけでなく，診断結果に至った根拠の提示が，本研究が次に取り\\n組むべき課題である．\\n\\n33\\n\\n\\n\\n謝辞\\n\\n本研究の全過程を通して，懇切なる御指導を頂きました知的情報処理研究室 彌冨 仁 准教\\n授に心より感謝致します．そして，日頃より本研究に対して多くの御意見を寄せてください\\nました彌冨研究室の皆様に深く御礼を申し上げます．\\n\\n34\\n\\n\\n\\n参考文献\\n\\n[1] Australian Institute of Health and Welfare. “Australian Cancer Incidence and Mortality\\n\\nbooks 2015” ＜ http://www.aihw.gov.au/acim-books/＞, Jan. 2015.\\n\\n[2] L. H. Sobin, M. K. Gospodarowicz and Ch. Wittekind. “UICC: Skin Tumours. TNM\\n\\nClassification of Malignant Tumors, 6th Edition,” New York: Wiley-Blackwell, pp.123-\\n\\n130, 2002.\\n\\n[3] F. L. Meyskens Jr., D. H. Berdeaux, B. Parks, T. Tong, L. Loescher and T. E. Moon.\\n\\n“Natural history and prognostic factors influencing survival in patients with stage I\\n\\ndisease,” Cancer 1998; Vol.62, Vo.2, pp.1207-1214, Sep. 1988.\\n\\n[4] H. P. Soyer, J. Smolle, H. Kerl, and H. Stettner. “Early diagnosis of malignant melanoma\\n\\nby surface microscopy,” Lancet, Vol.2, No.8562, p.803, Oct. 1987.\\n\\n[5] M. Tanaka. “Dermoscopy,” Journal of Dermatology, Vol.3, pp.513-517, Aug. 2006.\\n\\n[6] W. Stolz, A. Riemann, A. B. Cognetta, L. Pillet, W. Abmayr, and D. Holzel. “ABCD\\n\\nrule of dermatoscopy: a new practical method for early recognition of malignant\\n\\nmelanoma,” European Journal of Dermatology, No.4, No.7, pp.521-527, Apr. 1994.\\n\\n[7] S. W. Menzies, K. A. Crotty and W. H. McCarthy. “Frequency and morphologic charac-\\n\\nteristics of invasive melanomas lacking specific surface microscopic features,” Archives\\n\\nof Dermatology, No.132, pp.1178-1182, 1996.\\n\\n[8] G. Argenziano, G. Fabbrocini, P. Carli, V. De. Giorgi, E. Sammarco and M. Delfino.\\n\\n“Epiluminescence microscopy for the diagnosis of ABCD rule of dermatoscopy and a\\n\\nnew 7-point checklist based on pattern analysis,” Archives of Dermatology, No.134,\\n\\npp.1536-1570, 1998.\\n\\n[9] A. Blum, G. Rassner and C. Garbe. “Modified ABC-point list of dermoscopy: A\\n\\nsimplified and highly accurate dermoscopic algorithm for the diagnosis of cutaneous\\n\\nmelanocytic lesions,” Journal of the Americal Academy of Dermatology, Vol.48, No.5,\\n\\npp.672-678, 2003.\\n\\n35\\n\\n\\n\\n[10] H. P. Soyer, G. Argenziano, I. Zalaudek, R. Corona, F. Sera, R. Talamini, F. Barbato,\\n\\nA. Baroni, L. Cicale, A. Di. Stefani, P. Farro, L. Rossiello, E. Ruocco and S. Chimenti.\\n\\n“Three-Point Checklist of Dermoscopy: A New Screening Method for Early Detection\\n\\nof Melanoma,” Dermatology, Vol.208, pp.27-31, 2004.\\n\\n[11] J. Mayer. “Systematic review of the diagnostic accuracy of dermoscopy in detecting\\n\\nmalignant melanoma,” Med. Journal of Australia, Vol.167, No.4, pp.206-210, Aug. 1997.\\n\\n[12] W. Stolz, O. B. Falco, P. Bilek, M. Landthaler, W. H. C. Burgdorf and A. B. Cognetta.\\n\\n“Color Atlas of Dermatoscopy – 2nd enlarged and completely revised edition,” Berlin,\\n\\nBlackwell publishing, 2002.\\n\\n[13] G. Argenziano, H. P. Soyer, S. Chimenti, R. Talamini, R. Corona, F. Sara, M. Binder,\\n\\nL. Cerroni, G. De. Rosa, G. Ferrara, R. Hofmann-Wellenhof, M. Landthaler, S. W.\\n\\nMenzies, H. Pehamberger, D. Piccolo, H. S. Rabinovitz, R. Schiffner, S. Staibano, W.\\n\\nStolz, I. Bartenjev, A. Blum, R. Braun, H. Cabo, P. Carli, V. De. Giorgi, M. G. Fleming,\\n\\nJ. M. Grichnik, C. M. Grin, A. C. Halpern, R. Johr, B. Katz, R. O. Kenet, H, Kittler,\\n\\nJ. Kreusch, J. Malvehy, G. Mazzocchetti, M. Oliviero, F. Ozdemir, K. Peris, R. Perotti,\\n\\nA. Perusquia, M. A. Pizzichetta, S. Puig, B. Rao, P. Rubegni, T. Saida, M. Scalvenzi,\\n\\nS. Seidenari, I. Stanganelli, M. Tanaka, K. Westerhoff, I. H. Wolf, O. Braun-Falco, H.\\n\\nKerl, T. Nishikawa, K. Wolff and A. W. Kopf. “Dermoscopy of pigmented skin lesions:\\n\\nResults of a consensus meeting via the Internet,” Journal of American Academy of\\n\\nDermatology, Vol.48, No.5, pp.679-693, May. 2003.\\n\\n[14] A. Blum, H. Luedtke, U. Ellwanger, R. Schwabe, G. Rassner and C. Garbe. “Digital Im-\\n\\nage Analysis for Diagnosis of Cutaneous Melanoma. Development of a Highly Effective\\n\\nComputer Algorithm Based on Analysis of 837 Melanocytic Lesions,” British Journal\\n\\nof Dermatology, Vol.151, pp.1029-1038, 2004.\\n\\n[15] H. Ganster, A. Pinz, R. Rohrer, E. Wilding, M. Binder and H. Kitter. “Automated\\n\\nmelanoma recognition,” IEEE Trans. on Medical Imaging, Vol.20, No.3, pp.233-239,\\n\\nMar. 2001.\\n\\n[16] M. Elbaum, A. W. Kopf, H. S. Rabinovitz, R. G. Langley, H. Kamino, M. C. Mihm Jr,\\n\\nA. J. Sober, G. L. Peck, A. Bogdan, D. Gutkowicz-Krusin, M. Greenebaum, S. Keem,\\n\\nM. Oliviero and S. Wang. “Automatic differentiation of melanoma from melanocytic\\n\\nnevi with multispectral digital dermoscopy: a feasibility study,” Journal of American\\n\\nAcademy of Dermatology, Vol.44, pp.207-218, Feb. 2001.\\n\\n36\\n\\n\\n\\n[17] P. Rubegni, G. Cevenini, M. Burroni, R. Perotti, G. Dell’Eva, P. Sbano, C. Miracco,\\n\\nP. Luzi, P. Tosi, P. Barbini and L. Andreassi. “Automated diagnosis of pigmented skin\\n\\nlesions,” International Journal of Cancer, Vol.101, pp.576-580, Oct. 2002.\\n\\n[18] K. Hoffmann, T. Gambichler, A. Rick, M. Kreutz, M. Anschuetz, T. Grunendick, A.\\n\\nOrlikov, S. Gehlen, R. Perotti, L. Andreassi, J. Newton Bishop, J. P. Cesarini, T.\\n\\nFischer, P. J. Frosch, R. Lindskov, R. Mackie, D. Nashan, A. Sommer, M. Neumann,\\n\\nJ. P. Ortonne, P. Bahadoran, P. F. Penas, U. Zoras and P. Altmeyer. “Diagnostic\\n\\nand neural analysis of skin cancer (DANAOS). A multicentre study for collection and\\n\\ncomputer-aided analysis of data from pigmented skin lesions using digital dermoscopy,”\\n\\nBritish Journal of Dermatology, Vol.149, pp.801-809, Oct. 2003.\\n\\n[19] H. Oka, M. Hashimoto, H. Iyatomi and M. Tanaka. “Internet-based program for au-\\n\\ntomatic discrimination of dermoscopic images between melanomas and Clark naevi,”\\n\\nBritish Journal of Dermatology, No.150, No.5, p.1041, May. 2004.\\n\\n[20] M. Burroni, P. Sbano, G. Cevenini, M. Risulo, G. Dell’Eva, P. Barbini, C. Miracco, M.\\n\\nFimiani, L. Andreassi and P. Rubegni. “Dysplastic naevus vs. in situ melanoma: digital\\n\\ndermoscopy analysis,” British Journal of Dermatology, Vol.152, pp.679-684, Apr. 2005.\\n\\n[21] S. Seidenari, G. Pellacani and C. Grana. “Pigment distribution in melanocytic lesion im-\\n\\nages: a digital parameter to be employed for computer-aided diagnosis,” Skin Research\\n\\nand Technology, Vol.11, pp.236-241, Nov. 2005.\\n\\n[22] S. W. Menzies, L. Bischof, H. Talbot, A. Gutener, M. Avramidis, L. Wong, S. K. Lo,\\n\\nG. Mackellar, V. Skladnev, W. McCarthy, J. Kelly, B. Cranney, P. Lye, H. Rabinovitz,\\n\\nM. Oliviero, A. Blum, A. Varol, B. De’Ambrosis, R. McCleod, H. Koga, C. Grin, R.\\n\\nBraun and R. Johr. “The performance of SolarScan - An. automated dermoscopy image\\n\\nanalysis instrument for the diagnosis of primary melanoma,” Archives of Dermatology,\\n\\nVol.141, No.11, pp.1388-1396, Nov. 2005.\\n\\n[23] M. E. Celebi, H. A. Kingravi, B. Uddin, H. Iyatomi, Y. A. Aslandogan, W. V. Stoecker,\\n\\nand R. H. Moss. “A methodological approach to the classification of dermoscopy im-\\n\\nages,” Computerized Medical Imaging and Graphics, Vol.31, No.6, pp.362-373, 2007.\\n\\n[24] H. Iyatomi, H. Oka, M. E. Celebi, M. Hashimoto, M. Hagiwara, M. Tanaka and K.\\n\\nOgawa. “An improved Internet-based melanoma screening system with dermatologist-\\n\\nlike tumor area extraction algorithm,” Computerized Medical Imaging and Graphics,\\n\\nVol.32, No.7, pp.566-579, 2008.\\n\\n37\\n\\n\\n\\n[25] A. Tenenhaus, A. Nkengne, J. F. Horn, C. Serruys, A. Giron and B. Fertil. “Detection of\\n\\nmelanoma from dermoscopic images of naevi acquired under uncontrolled conditions,”\\n\\nSkin Research and Technology, Vol.16, pp.85-97, 2010.\\n\\n[26] Z. Liu, J. Sun, L. Smith, M. Smith, and R. Warr. “Distribution quantification on\\n\\ndermoscopy images for computer-assisted diagnosis of cutaneous melanomas,” Medical\\n\\n& biological engineering & computing, Vol.50, No.5, pp.503-513, 2012.\\n\\n[27] K. Shimizu, H. Iyatomi, M. E. Celebi, K. Norton and M. Tanaka. “Four-class clas-\\n\\nsification of skin lesions with task decomposition strategy,” IEEE Trans. Biomedical\\n\\nEngineering, Vol.62, No.1, pp.274-283, 2015.\\n\\n[28] K. Korotkov and R. Garcia. “Computerized Analysis of Pigmented Skin Lesions: A\\n\\nReview,” Artificial Intelligence in Medicine, Vol. 56, No. 2, pp.69-90, 2012.\\n\\n[29] M. E. Celebi, Q. Wen, H. Iyatomi and G. Schaefer. “A State-of-the-Art Survey on Lesion\\n\\nBorder Detection in Dermoscopy Images,” Dermoscopy Image Analysis, Publisher: CRC\\n\\nPress, pp.97-129, Sep. 2015.\\n\\n[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpa-\\n\\nthy, A. Khosla, M. Bernstein, A. C. Berg and L. Fei-Fei. “ImageNet Large Scale Vi-\\n\\nsual Recognition Challenge,” International Journal of Computer Vision, Vol.115, No.3,\\n\\npp.211-252, 2015.\\n\\n[31] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. “OverFeat:\\n\\nIntegrated Recognition, Localization and Detection using Convolutional Networks,”\\n\\nCoRR, abs/1312.6229, 2013.\\n\\n[32] S. Ioffe and C. Szegedy. “Batch Normalization: Accelerating Deep Network Training by\\n\\nReducing Internal Covariate Shift,” CoRR, abs/1502.03167, 2015.\\n\\n[33] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke\\n\\nand A. Rabinovich. “Going Deeper With Convolutions,” CoRR, abs/1409.4842, 2014.\\n\\n[34] K. Simonyan and A. Zisserman. “Very Deep Convolutional Networks for Large-Scale\\n\\nImage Recognition,” CoRR, abs/1409.1556, 2014.\\n\\n[35] Y. LeCun, L. Bottou, Y. Bengio and P. Haffner. “Gradient-based learning applied to\\n\\ndocument recognition,” Proc. IEEE, Vol.86, No.11, pp.2278-2324, 1998.\\n\\n[36] K. Jarrett, K. Kavukcuoglu, M. Ranzato and Y. LeCun. “What is the best multi-stage\\n\\narchitecture for object recognition?,” Computer Vision, pp.2146-2153, 2009.\\n\\n38\\n\\n\\n\\n[37] A. Krizhevsky, I. Sutskever and G. E. Hinton. “ImageNet Classification with Deep\\n\\nConvolutional Neural Networks,” Advances in Neural Information Processing Systems,\\n\\npp.1106-1114, 2012.\\n\\n[38] N. Codella, J. Cai, M. Abedini, R. Garnavi, A. Halpern, and J. R. Smith, “Deep\\n\\nlearning, sparse coding, and SVM for melanoma recognition in dermoscopy images,” in\\n\\nMICCAI MLMI, Vol.9352, pp.118-126, 2015.\\n\\n[39] J. Kawahara, A. BenTaieb, and G. Hamarneh, “Deep Features to Classify Skin Lesions,”\\n\\nIEEE 13th International Symposium on Biomedical Imaging, pp.1397-1400, 2016.\\n\\n[40] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau and S. Thrun.\\n\\n“Dermatologist-level classification of skin cancer with deep neural networks,” Nature,\\n\\nVol.542, pp.115-118, Feb 2017.\\n\\n[41] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. “ImageNet: A large-scale\\n\\nhierarchical image database,” Proc. IEEE Computer Society Conference on Computer\\n\\nVision and Pattern Recognition, pp.2-9, 2009.\\n\\n[42] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna. “Rethinking the Inception\\n\\nArchitecture for Computer Vision,” Proc. IEEE Computer Society Conference Com-\\n\\nputer Vision and Pattern Recognition, abs/1512.00567, 2015.\\n\\n[43] S. J. Pan and Q. Yang. “A Survey on Transfer Learning,” IEEE Transactions on Knowl-\\n\\nedge and Data Engineering, Vol.22, No.10, pp.1345-1359, 2010.\\n\\n[44] D. Gutman, N. Codella, E. Celebi, B. Helba, M. Marchetti, N. Mishra and A. Halpern.\\n\\n“Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International\\n\\nSymposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imag-\\n\\ning Collaboration (ISIC),” Proc. IEEE Computer Society Conference Computer Vision\\n\\nand Pattern Recognition, abs/1605.01397, 2016.\\n\\n[45] T. Yoshida, M. E. Celebi, G. Schaefer and H. Iyatomi “Simple and Effective Pre-\\n\\nprocessing for Automated Melanoma Discrimination based on Cytological Findings,”\\n\\nProc. IEEE Big Data 2016 (3rd Big Data Analytic Technology for Bioinformatics and\\n\\nHealth Informatics (KDDBHI2016)), pp.3439-3442, Dec. 2016.\\n\\n[46] A. F. Frangi, W. J. Niessen, K. L. Vincken and M. A. Viergerver. “Multiscale vessel\\n\\nenhancement filtering,” Medical Image Computing and Computer Assisted Intervention,\\n\\nVol.1496, pp.130-137, 1998.\\n\\n39\\n\\n\\n\\n[47] Y. Sato, S. Nakajima, N. Shiraga, H. Atsumi, S. Yoshida, T. Koller, G. Gerig and R.\\n\\nKikinis. “Three-dimensional multi-scale line filter for segmentation and visualization of\\n\\ncurvatures in medical images,” Medical Image Analysis, Vol.2, No.2, pp.143-168, 1998.\\n\\n[48] M. Bertalmio, A. L. Bertozzi, and G. Sapiro. “Navier-stokes, Fluid Dynamics, and\\n\\nImage and Video Inpainting,” Proc. IEEE Computer Society Conference on Computer\\n\\nVision and Pattern Recognition, pp.355-362, 2001.\\n\\n[49] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama\\n\\nand T. Darrell. “Caffe: Convolutional Architecture for Fast Feature Embedding,” arXiv\\n\\npreprint arXiv: 1408.5093, 2014.\\n\\n40\\n\\n\\n\\n業績\\n\\n• 国際会議\\n\\n1. T. Yoshida, M. E. Celebi, G. Schaefer and H. Iyatomi “Simple and Effective Pre-\\n\\nprocessing for Automated Melanoma Discrimination based on Cytological Find-\\n\\nings,” Proc. IEEE Big Data 2016 (3rd Big Data Analytic Technology for Bioin-\\n\\nformatics and Health Informatics (KDDBHI2016)), pp.3439-3442, Dec. 2016.\\n\\n• 国内会議\\n\\n2. 吉田 拓也, 彌冨 仁. “深層畳み込みニューラルネットワークを用いたメラノーマ\\n自動診断システムの試作,” 電子情報通信学会総合大会講演論文集 2015年 情報・\\nシステム (2), P.148, Mar. 2015.\\n\\n3. 吉田 拓也, 彌冨 仁. “深層畳み込みニューラルネットワークを用いたメラノーマ\\n自動識別器の構築,” 日本知能情報ファジィ学会ファジィシステムシンポジウム講\\n演論文集, Vol.31, pp.379-382, Sep. 2015.\\n\\n4. 吉田 拓也, 彌冨 仁. “畳み込みニューラルネットワークを用いたメラノーマ自動\\n識別,” 電子情報通信学会総合大会講演論文集 2016年 情報・システム (2), P.173,\\nMar. 2016.\\n\\n• その他学術的活動\\n\\n5. D. Shimada, T. Yoshida, H. Iyatomi and M. E. Celebi (Hosei Univ. ConvNet\\n\\nTeam). Skin Lesion Analysis toward Melanoma Detection Part 1: Lesion Seg-\\n\\nmentation, A Challenge at the International Symposium on Biomedical Imaging\\n\\n(ISBI), 2016.\\n\\n6. D. Shimada, T. Yoshida, H. Iyatomi and M. E. Celebi (Hosei Univ. ConvNet\\n\\nTeam). Skin Lesion Analysis toward Melanoma Detection Part 3: Lesion Clas-\\n\\nsification, A Challenge at the International Symposium on Biomedical Imaging\\n\\n(ISBI), 2016.\\n\\n41\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_data = parser.from_file(\"./input/ファイル3.pdf\")\n",
    "text = file_data[\"content\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "THE M5 COMPETITION\n",
      "Competitors’ Guide\n",
      "\n",
      "\n",
      "Objectives\n",
      "The objective of the M5 forecasting competition is to advance the theory and practice of forecasting by identifying the method(s) that provide the most accurate point forecasts for each of the 42,840 time series of the competition. I addition, to elicit information to estimate the uncertainty distribution of the realized values of these series as precisely as possible. \n",
      "To that end, the participants of M5 are asked to provide 28 days ahead point forecasts (PFs) for all the series of the competition, as well as the corresponding median and 50%, 67%, 95%, and 99% prediction intervals (PIs).\n",
      "The M5 differs from the previous four ones in five important ways, some of them suggested by the discussants of the M4 competition, as follows:\n",
      "First, it uses grouped unit sales data, starting at the product-store level and being aggregated to that of product departments, product categories, stores, and three geographical areas: the States of California (CA), Texas (TX), and Wisconsin (WI).\n",
      "Second, besides the time series data, it includes explanatory variables such as sell prices, promotions, days of the week, and special events (e.g. Super Bowl, Valentine’s Day, and Orthodox Easter) that typically affect unit sales and could improve forecasting accuracy.\n",
      "Third, in addition to point forecasts, it assesses the distribution of uncertainty, as the participants are asked to provide information on nine indicative quantiles.\n",
      "Fourth, instead of having a single competition to estimate both the point forecasts and the uncertainty distribution, there will be two parallel tracks using the same dataset, the first requiring 28 days ahead point forecasts and the second 28 days ahead probabilistic forecasts for the median and four prediction intervals (50%, 67%, 95%, and 99%).\n",
      "Fifth, for the first time it focuses on series that display intermittency, i.e., sporadic demand including zeros.\n",
      "Dates and hosting\n",
      "The M5 will start on March 2, 2020 and finish on June 30 of the same year. The competition will be run using the Kaggle platform. Thus, we expect many submissions from all types of forecasters including data scientists, statisticians, and practitioners, expanding the field of forecasting and eventually integrating its various approaches for improving accuracy and uncertainty estimation.\n",
      "\n",
      "The competition will be divided into two separate Kaggle competitions, using the same dataset, with the first (M5 Forecasting Competition – Accuracy) requiring 28 days ahead point forecasts and the second (M5 Forecasting Competition – Uncertainty) 28 days ahead probabilistic forecasts for the corresponding median and four prediction intervals (50%, 67%, 95%, and 99%).\n",
      "\n",
      "In order to support the participants to validate their forecasting approaches, the competition will include a validation phase that will take place from March 2, 2020 to 31 May of the same year. During this phase, the participants will be allowed to train their forecasting methods with the data initially provided by the organizers and validate the performance of their approaches using a hidden sample of 28 days, not made publicly available. By submitting their forecasts at the Kaggle platform (a maximum of 5 entries per day), the participants will be informed about the score of their submission, which will be then published in Kaggle’ s real-time leaderboard. Given this instant feedback, participants will be allowed to effectively revise and resubmit their forecasts by learning from the received feedback.\n",
      "\n",
      "After the end of the validation phase, i.e., from June 1, 2020 to 30 June of the same year, the participants will be provided with the actual values of the 28 days of data used for scoring their performance during the validation phase. They will be asked then to re-estimate or adjust (if needed) their forecasting models in order to submit their final forecasts and prediction intervals for the following 28 days, i.e., the data used for the final evaluation of the participants. During this time, there will be no leaderboard, meaning that no feedback will be given to the participants about their score after submitting their forecasts. Thus, although the participants will be free to (re)submit their forecasts any time they wish (a maximum of 5 entries per day), they will not be aware of their absolute, as well as their relative performance. The final ranks of the participants will be made available only at the end of competition, when the test data will be made available. This is done in order for the competition to simulate reality as closely as possible, given that in real life forecasters do not know the future. \n",
      "\n",
      "Note that the submission system will be open from the beginning of the competition, meaning that participants will be able to submit their final forecast from March 2, 2020 to June 30, 2020, even during the validation phase. However, as previously mentioned, the complete M5 training sample (including the 28 days used for the validation phases’ leaderboard) will only become available on June 1, 2020. So, any participant submitting his/his/their final forecasts during the validation phase will be missing the last 28 days of the complete training sample.\n",
      "\n",
      "Note also that M5 will be divided into two tracks, one requiring point forecasts, and one requiring the estimation of the uncertainty distribution, each with its separate prizes of $50,000. Thus, two individual competitions will be visible at the Kaggle platform, each one with its own separate leaderboard. Participants are allowed to compete and be eligible for prizes in the first track, the second track, or both. \n",
      "The dataset\n",
      "The M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI). In this respect, the bottom-level of the hierarchy, i.e., product-store unit sales can be mapped across either product categories or geographical regions, as follows:\n",
      "Table 1: Number of M5 series per aggregation level.\n",
      "\n",
      "\n",
      "Figure 1: An overview of how the M5 series are organized.\n",
      "The historical data range from 2011-01-29 to 2016-06-19. Thus, the products have a (maximum) selling history of 1,941 days / 5.4 years (test data of h=28 days not included). \n",
      "\n",
      "The M5 dataset consists of the following three (3) files:\n",
      "\n",
      "File 1: “calendar.csv” \n",
      "Contains information about the dates the products are sold.\n",
      "date: The date in a “y-m-d” format.\n",
      "wm_yr_wk: The id of the week the date belongs to.\n",
      "weekday: The type of the day (Saturday, Sunday, …, Friday).\n",
      "wday: The id of the weekday, starting from Saturday.\n",
      "month: The month of the date.\n",
      "year: The year of the date.\n",
      "event_name_1: If the date includes an event, the name of this event.\n",
      "event_type_1: If the date includes an event, the type of this event.\n",
      "event_name_2: If the date includes a second event, the name of this event.\n",
      "event_type_2: If the date includes a second event, the type of this event.\n",
      "snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n",
      "File 2: “sell_prices.csv”\n",
      "Contains information about the price of the products sold per store and date.\n",
      "store_id: The id of the store where the product is sold. \n",
      "item_id: The id of the product.\n",
      "wm_yr_wk: The id of the week.\n",
      "sell_price: The price of the product for the given week/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set).  \n",
      "File 3: “sales_train.csv” \n",
      "Contains the historical daily unit sales data per product and store.\n",
      "item_id: The id of the product.\n",
      "dept_id: The id of the department the product belongs to.\n",
      "cat_id: The id of the category the product belongs to.\n",
      "store_id: The id of the store where the product is sold.\n",
      "state_id: The State where the store is located.\n",
      "d_1, d_2, …, d_i, … d_1941: The number of units sold at day i, starting from 2011-01-29. \n",
      "Evaluation\n",
      "Forecasting horizon\n",
      "The number of forecasts required, both for point and probabilistic forecasts, is h=28 days (4 weeks ahead). \n",
      "\n",
      "The performance measures are first computed for each series separately by averaging their values across the forecasting horizon and then averaged again across the series in a weighted fashion (see below) to obtain the final scores. \n",
      "\n",
      "Point forecasts\n",
      "The accuracy of the point forecasts will be evaluated using the Root Mean Squared Scaled Error (RMSSE), which is a variant of the well-known Mean Absolute Scaled Error (MASE) proposed by Hyndman and Koehler (2006). The measure is calculated for each series as follows:\n",
      "\n",
      "where  is the actual future value of the examined time series at point t,  the generated forecast, n the length of the training sample (number of historical observations), and h the forecasting horizon. \n",
      "\n",
      "Note that the denominator of RMSSE is computed only for the time-periods for which the examined product(s) are actively sold, i.e., the periods following the first non-zero demand observed for the series under evaluation.   \n",
      "\n",
      "The choice of the measure is justified as follows:\n",
      "The M5 series are characterized by intermittency, involving sporadic unit sales with lots of zeros. This means that absolute errors, which are optimized for the median, would assign lower scores (better performance) to forecasting methods that derive forecasts close to zero. However, the objective of M5 is to accurately forecast the average demand and for this reason, the accuracy measure used builds on squared errors, which are optimized for the mean. \n",
      "The measure is scale independent, meaning that it can be effectively used to compare forecasts across series with different scales. \n",
      "In contrast to other measures, it can be safely computed as it does not rely on divisions with values that could be equal or close to zero (e.g. as done in percentage errors when  or relative errors when the error of the benchmark used for scaling is zero).\n",
      "The measure penalizes positive and negative forecast errors, as well as large and small forecasts, equally, thus being symmetric.\n",
      "After estimating the RMSSE for all the 42,840 time series of the competition, the participating methods will be ranked using the Weighted RMSSE (WRMSSE), as described latter in this Guide, using the following formula:\n",
      "\n",
      "where  is the weight of the  series of the competition. A lower WRMSSE score is better.\n",
      "Note that the weight of each series will be computed based on the last 28 observations of the training sample of the dataset, i.e., the cumulative actual dollar sales that each series displayed in that particular period (sum of units sold multiplied by their respective price). An indicative example for computing the WRMSSE will be available on the GitHub repository of the competition. \n",
      "Probabilistic forecasts\n",
      "The precision of the probabilistic forecasts will be evaluated using the Scaled Pinball Loss (SPL) function. The measure is calculated for each series and quantile as follows:\n",
      "\n",
      "\n",
      "where  is the actual future value of the examined time series at point t,  the generated forecast for quantile u, h the forecasting horizon, n the length of the training sample (number of historical observations), and 1 the indicator function (being 1 if Y is within the postulated interval and 0 otherwise).\n",
      "\n",
      "As done with RMSSE, the denominator of SPL is computed only for the time-periods for which the examined items/products are actively sold, i.e., the periods following the first non-zero demand observed for the series under evaluation.   \n",
      "\n",
      "Given that forecasters will be asked to provide the median, and the 50%, 67%, 95%, and 99% PIs,  is set to u1=0.005, u2=0.025, u3=0.165, u4=0.25, u5=0.5, u6=0.75, u7=0.835, u8=0.975, and u9=0.995. The smaller values of u correspond to the left side of the distribution, while the higher values to the right side of the distribution, with u = 0.5 being the median. The median and the 50% and 67% PIs provide a good sense of the middle of the distribution, while the 95% and 99% PIs provide information about its tails, which are important in terms of the risk of extremely high or extremely low outcomes.\n",
      "After estimating the SPL for all the 42,840 time series of the competition and for all the requested quantiles, the participating methods will be ranked using the Weighted SPL (WSPL), as described latter in this Guide, divided by nine (average performance of nine quantiles across all series) , using the following formula:\n",
      "\n",
      "where  is the weight of the  series of the competition and  the out of the examined quantiles. A lower WSPL score is better.\n",
      "The choice of the measure is justified as follows:\n",
      "PL is scaled in a similar fashion to that of RMSSE, meaning that it can be effectively used to compare forecasts across series with different scales. Moreover, SPL can be safely computed as it does not rely on divisions with values that could be equal to zero.\n",
      "Since M5 does not focus on a particular decision-making problem, neither defines the exact parameters of such a problem (which could also vary for different aggregation levels and series), it becomes evident that all quantiles could be potentially useful.  Moreover, since the objective of the M5 is to estimate the uncertainty distribution of the realized values of the examined series as precisely as possible, both sides and both ends of the distribution are considered relevant. In this regard, no special weights are assigned to the examined quantiles, which are therefore equally weighted.\n",
      "Note that, once again, the weight of each series will be computed based on the last 28 observations of the training sample of the dataset, i.e., the cumulative actual dollar sales that each series displayed in that particular period (sum of units sold multiplied by their respective price). An indicative example for computing the WSPL will be available on the GitHub repository of the competition.\n",
      "Weighting \n",
      "In contrast to the previous M competition, M5 involves the unit sales of various products of different selling volumes and prices that are organized in a hierarchical fashion. This means that, businesswise, in order for a method to perform well, it must provide accurate forecasts across all hierarchical levels, especially for series of high importance, i.e. for series that represent significant sales, measured in US dollars. In other words, we expect from the best performing forecasting methods to derive lower forecasting errors for the series that are more valuable for the company. \n",
      "\n",
      "To that end, the forecasting errors computed for each participating method (both RMSSE and SPL) will be weighted across the M5 series based on their cumulative actual dollar sales, which is a good and objective proxy of their actual value for the company in monetary terms. The cumulative dollar sales will be computed using the last 28 observations of the training sample (sum of units sold multiplied by their respective price), i.e., a period equal to the forecasting horizon. Note that since both the number of units being sold and their respective price change through time, this estimation is based on the sum of the corresponding daily dollar sales.\n",
      "\n",
      "Below you may find a simple, yet indicative example of how these weights will be computed:\n",
      "\n",
      "Assume that two products of the same department, A and B, are sold in a store at WI and we are interested in forecasting the unit sales of these two products, as well as their aggregate sales. Thus, in this example, we consider two different aggregation levels (K=2), the first level consisting of two series (series A and B) and the second level of a single series (sum of series A and B). \n",
      "\n",
      "Product A displayed a total of $10 in sales in the last 28 days of the training sample, while product B $12. Thus, the aggregate dollar sales of products A and B in the last 28 days were $22. Assume also that a forecasting method was used to derive point forecasts for product A, product B, and their aggregate unit sales, displaying errors RMSSEA=0.8, RMSSEB=0.7, and RMSSE=0.77, respectively. If the M5 dataset involved just those three series, the final WRMSSE score of the method would be\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "This weighting scheme can be expanded in order to consider more stores, geographical regions, product categories, and product departments, as previously described. Since the M5 competition involves twelve aggregation levels, K is set equal to 12, with the weights of the series being computed so that they sum to one at each aggregation level.  \n",
      "\n",
      "Respectively, RMSSE, which is used in the above equation for estimating WRMSSE, can be replaced with SPL to compute WSPL.\n",
      "\n",
      "Note that all hierarchical levels are equally weighted. The reason is because the total dollar sales of a product, measured across all three States, are equal to the sum of the dollar sales of this product when measured across all ten stores. Similarly, because the total dollar sales of a product category of a store are equal to the sum of the dollar sales of the departments that this category consists of, as well as the sum of the dollar sales of the products of the corresponding departments. Moreover, as previously discussed for the case of the probabilistic forecasts, M5 does not focus on a particular decision-making problem, which means that there is no reason for weighting unequally the individual levels of the hierarchy.  \n",
      "An indicative example for computing WRMSSE and WSPL will be available on the GitHub repository of the competition, indicating among others the exact weight of each series in the competition.\n",
      "The Prizes\n",
      "Distribution of prize money\n",
      "There will be 12 major prizes awarded to the winners of the M5 Competition, which will be further distributed among the participants based on (i) the hierarchical levels that their forecasts exceled and (ii) the quantiles of the uncertainty distribution that were better captured. The Prizes will be awarded on December 8, 2020, during the M5 Forecasting Conference to be held in New York City. At this date, Kaggle will be issuing the payments digitally using its collaborating firm Payoneer.\n",
      "The total of the $100,000 prize money will be distributed equally between the Forecasting and Uncertainty M5 competition as follows:\n",
      "\n",
      "Reproducibility\n",
      "The prerequisite for winning any prize will be that the code used for generating the forecasts, with the exception of companies providing forecasting services and those claiming proprietary software, will be put on GitHub, not later than 14 days after the end of the competition (i.e., the 14th of July, 2020). In addition, there must be instructions on how to exactly reproduce the M5 submitted forecasts. In this regard, individuals and companies will be able to use the code and the instructions provided, crediting the person/group that has developed them, to improve their organizational forecasts.\n",
      "\n",
      "Companies providing forecasting services and those claiming proprietary software will have to provide the organizers with a detailed description of how their forecasts were made and a source, or execution file for reproducing their forecasts. Given the critical importance of objectivity and replicability, such description and file will be mandatory for winning any prize of the competition. An execution file can be submitted in case that the source program needs to be kept confidential, or, alternatively, a source program with a termination date for running it.\n",
      "\n",
      "After receiving the code/program/files for reproducing the submitted forecasts, the organizers will evaluate their results in terms of reproducibility. Since some methods may involve random initializations, any method that displays a replicability rate higher than 98% will be considered as fully replicable and be awarded the prize, exactly as done in M4. Otherwise, the prize will be given to the next best-performing and fully reproducible submission.\n",
      "Publications\n",
      "Similar to the M3 and M4 competitions, there will be a special issue of the International Journal of Forecasting (IJF) exclusively devoted to all aspects of the M5 Competition with special emphasis on what we have learned and how we can use such learning to improve the theory and practice of forecasting as well as expand its usefulness and applicability.\n",
      "The Benchmarks\n",
      "Like done in the M4 competition, there will be benchmark methods, twenty-four (24) for point forecasts, and six (6) for probabilistic ones. As these methods are well known, readily available, and straightforward to apply, the accuracy of the new ones submitted to the M5 Competition must provide superior accuracy in order to be considered and used in practice (taking also into account the computational time it would be required to utilize a more accurate method versus the benchmarks whose computational requirements are minimal).\n",
      "\n",
      "Point forecasts\n",
      "Statistical Benchmarks\n",
      " \n",
      "1. Naive: A random walk model, defined as \n",
      "\n",
      "The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "2. Seasonal Naive (sNaive): Like Naive, but this time the forecasts of the model are equal to the last known observation of the same period in order for it to capture possible weekly seasonal variations. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "3. Simple Exponential Smoothing (SES): The simplest exponential smoothing model, aimed at predicting series without a trend, defined as\n",
      "\n",
      "The smoothing parameter a is selected from the range [0.1, 0.3] by minimizing the insample mean squared error (MSE) of the model, while the first observation of the series is used for initialization. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "4. Moving Averages (MA): Forecasts are computed by averaging the last k observations of the series, as follows\n",
      "\n",
      "where k is selected from the range [2, 5] by minimizing the insample MSE. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "5. Croston’s method (CRO): The method proposed by Croston to forecast series that display intermittent demand. The method decomposes the original series into the non-zero demand size  and the inter-demand intervals, deriving forecasts as follows:\n",
      "\n",
      "where both  and  are predicted using SES. The smoothing parameter of both components is set equal to 0.1. The first observation of the components are used for initialization. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "6. Optimized Croston’s method (optCro): Like CRO, but this time the smoothing parameter is selected from the range [0.1, 0.3], like done with SES, in order to allow for more flexibility. The non-zero demand size and the inter-demand intervals are smoothed separately using (potentially) different a parameters. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "7. Syntetos-Boylan Approximation (SBA): A variant of the Croston's method that utilizes a debiasing factor as follows:\n",
      "\n",
      "The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "8. Teunter-Syntetos-Babai method (TSB): A modification to Croston's method that replaces the inter-demand intervals component with the demand probability, , being 1 if demand occurs at time t and 0 otherwise. Similarly to Croston's method, is forecasted using SES. The smoothing parameters of  and may differ, exactly as optCRO. The forecast is given as follows:\n",
      "\n",
      "The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "9. Aggregate-Disaggregate Intermittent Demand Approach (ADIDA): Temporal aggregation is used for reducing the presence of zero observations, thus mitigating the undesirable effect of the variance observed in the intervals. ADIDA uses equally sized time buckets to perform non-overlapping temporal aggregation and predict the demand over a pre-specified lead-time. The time bucket is set equal to the mean inter-demand interval. SES is used to obtain the forecasts. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "10. Intermittent Multiple Aggregation Prediction Algorithm (iMAPA): Another way for implementing temporal aggregation in demand forecasting. However, in contrast to ADIDA that considers a single aggregation level, iMAPA considers multiple ones, aiming at capturing different dynamics of the data. Thus, iMAPA proceeds by averaging the derived point forecasts, generated using SES. The maximum aggregation level is set equal to the maximum inter-demand interval. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "11. Exponential Smoothing - Top-Down (ES_td): An algorithm is used to select the most appropriate exponential smoothing model for predicting the top-level series of the hierarchy (level 1 of Table 1), indicated through information criteria. The top-down method is used for reconciliation (based on historical proportions, estimated for the last 28 days). \n",
      "12. Exponential Smoothing – Bottom-Up (ES_bu): An algorithm is used to select the most appropriate exponential smoothing model for predicting the bottom-level series of the hierarchy (level 12 of Table 1), indicated through information criteria. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "13. Exponential Smoothing with eXplanatory variables (ESX): Similar to ES, but this time two explanatory variables are used as regressors to improve forecasting accuracy by providing additional information about the future.  The first variable is discrete and takes values 0, 1, 2 or 3, based on the number of States that allow SNAP purchases on the examined date. The second variable is binary and indicates whether the examined date includes a special event (1) or not (0). The top-down method is used for reconciliation (based on historical proportions, estimated for the last 28 days).\n",
      "14. AutoRegressive Integrated Moving Average - Top-Down (ARIMA_td): An algorithm is used to select the most appropriate ARIMA model for predicting the top-level series of the hierarchy (level 1 of Table 1), indicated through information criteria. The top-down method is used for reconciliation (based on historical proportions, estimated for the last 28 days).\n",
      "15. AutoRegressive Integrated Moving Average – Bottom-Up (ARIMA_bu): An algorithm is used to select the most appropriate ARIMA model for predicting the bottom-level series of the hierarchy (level 12 of Table 1), indicated through information criteria. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "16. AutoRegressive Integrated Moving Average with eXplanatory variables (ARIMAX): Similar to ARIMA, but this time two external variables are used as regressors to improve forecasting accuracy by providing additional information about the future, exactly as done for the case of ESX. The top-down method is used for reconciliation (based on historical proportions, estimated for the last 28 days).\n",
      "Machine Learning Benchmarks\n",
      "17. Multi-Layer Perceptron (MLP): A single hidden layer NN of 14 input nodes (last two weeks of available data), 28 hidden nodes, and one output node. The Scaled Conjugate Gradient method is used for estimating the weights that are initialized randomly, while the maximum iterations are set equal to 500. The activation functions of the hidden and output layers are the logistic and linear one, respectively. In total, 10 MLPs are trained to forecast each series and then the median operator is used to average the individual forecasts in order to mitigate possible variations due to poor weight initializations. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "18. Random Forest (RF): This is a combination of multiple regression trees, each one depending on the values of a random vector sampled independently and with the same distribution. Given that RF averages the predictions of multiple trees, it is more robust to noise and less likely to over-fit on the training data. We consider a total of 500 non-pruned trees and four randomly sampled variables at each split. Bootstrap sampling is done with replacement. Like done in MLP, the last 14 observations of the series are considered for training the model. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "19. Global Multi-Layer Perceptron (GMLP): Like MLP, but this time, instead of training multiple models, one for each series, a single model that learns across all series is constructed. This is done given that M4 indicated the beneficial effect of cross learning. The last 14 observations of each series are used as inputs, along with information about the coefficient of variation of non-zero demands (CV2) and the average number of time-periods between two successive non-zero demands (ADI). This additional information is used in order to facilitate learning across series of different characteristics. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "20. Global Random Forest (GRF): Like GMLP, but instead of using an MLP for obtaining the forecasts, a RF is exploited instead. The forecasting method is used for predicting the series of the lowest level of the hierarchy (level 12 of Table 1) and the bottom-up method is then used for reconciliation. \n",
      "Combination Benchmarks\n",
      "\n",
      "21. Average of ES and ARIMA, as computed using the bottom-up approach (Com_b): The simple arithmetic mean of ES_bu and ARIMA_bu.\n",
      "\n",
      "22. Average of ES and ARIMA, as computed using the top-down approach (Com_t): The simple arithmetic mean of ES_td and ARIMA_td.\n",
      "\n",
      "23. Average of the two ES methods, the first computed using the top-down approach and the second using the bottom-up approach (Com_tb): The simple arithmetic mean of ES_td and ES_bu.\n",
      "\n",
      "24. Average of the global and local MLPs (Com_lg): The simple arithmetic mean of MLP and GMLP. The bottom-up method is then used for reconciliation. \n",
      "Observe that the benchmark methods {1-10, 12, 15, 17-20} are applied at the product-store level of the hierarchically structured dataset. Thus, the bottom-up method is used for obtaining reconciled forecasts for the rest of the hierarchical levels. On the other hand, the benchmark methods {11, 13, 14, 16} are applied at the top level of the hierarchically structured dataset. Thus, the top-down method is used for obtaining reconciled forecasts for the rest of the hierarchical levels (based on historical proportions, estimated for the last 28 days). \n",
      "Probabilistic forecasts\n",
      "i. Naive: Similar implementation to the Naive 1 used for computing point forecasts.\n",
      "ii. Seasonal Naive (sNaive): Similar implementation to the sNaive one used for computing point forecasts.\n",
      "iii. Simple Exponential Smoothing (SES): Similar implementation to the SES one used for computing point forecasts.\n",
      "iv. Exponential Smoothing (ES): Similar implementation to the ES_bu one used for computing point forecasts.\n",
      "v. AutoRegressive Integrated Moving Average (ARIMA): Similar implementation to the ARIMA_bu one used for computing point forecasts.\n",
      "vi. Kerner density estimate (Kernel): A kernel is used to estimate the corresponding quantiles in the historical data that are then used as probabilistic forecasts.\n",
      "The code for generating the forecasts of the abovementioned benchmarks will be available on the GitHub repository of the competition. \n",
      "Benchmarks are not eligible for a prize, meaning that the total amount will be distributed among the competitors even if the benchmarks perform better than the forecasts submitted by the participants. Similarly, any participating method associated with the organizers and the data provider, will not be eligible for a price.\n",
      "Submission\n",
      "The forecasts for both competitions will be submitted through the Kaggle platform. The templates provided by the organizers though the platform can be used for this purpose.\n",
      "Note that the template of the point forecasts (M5 Forecasting - Accuracy) refers only to the 30,490 series that consist the lowest hierarchical level of the dataset (level 12 of Table 1) and not all 42,840 of the competition (all levels of Table 1). This is done because M5, in contrast to M4, M3, and other forecasting competitions where time series are mostly unrelated, deals among others with a real-life hierarchical forecasting problem. This means that the submitted forecasts must follow this hierarchical concept and, as a result, be coherent (forecasts at the lower levels have to sum up to the ones of the higher levels). In other words, it is assumed that the forecasting approach used for forecasting all 42,840 series of the competition derived coherent forecasts and, therefore, the forecasts of all levels can be automatically computed by aggregating (summing) the ones of the lowest level of the hierarchy. \n",
      "It is important to note that the participants are completely free to use the forecasting approaches of their choice for forecasting the individual series. However, having done that, and by submitting just the forecasts of the lowest level, it will be assumed that the derived forecasts were reconciled before submitted for the final evaluation. For instance, a participant may forecast just the series at the bottom-level and derive the remaining forecasts using the bottom-up reconciliation method. Another participant may forecast just the series at the top level and get the ones at the lower levels using proportions (top-down reconciliation method). A mix of the previous two approaches is also possible (middle-out reconciliation method). Finally, predicting the series of all levels and getting the ones of the lowest level through an appropriate weighting scheme is also an option. The benchmarks describe some of these options, involving some indicative forecasting approaches that utilize the bottom-up (e.g. benchmark #12) and the top-down (e.g. benchmark #11) reconciliation method, as well as the combination of these two (e.g. benchmark #23).  \n",
      "Finally, given that there is not a direct and well-established way for reconciling probabilistic forecasts, the template of the probabilistic forecasts (M5 Forecasting - Uncertainty) requires inputting all 42,840 series of the competition. Thus, in this case, participants do not need to reconcile the forecasts using any of the above-mentioned approaches. \n",
      "\n",
      "---------------------------\n",
      "Level \n",
      "id\n",
      "Aggregation Level\n",
      "Number of series\n",
      "1\n",
      "Unit sales of all products, aggregated for all stores/states\n",
      "1\n",
      "2\n",
      "Unit sales of all products, aggregated for each State\n",
      "3\n",
      "3\n",
      "Unit sales of all products, aggregated for each store \n",
      "10\n",
      "4\n",
      "Unit sales of all products, aggregated for each category\n",
      "3\n",
      "5\n",
      "Unit sales of all products, aggregated for each department\n",
      "7\n",
      "6\n",
      "Unit sales of all products, aggregated for each State and category\n",
      "9\n",
      "7\n",
      "Unit sales of all products, aggregated for each State and department\n",
      "21\n",
      "8\n",
      "Unit sales of all products, aggregated for each store and category\n",
      "30\n",
      "9\n",
      "Unit sales of all products, aggregated for each store and department\n",
      "70\n",
      "10\n",
      "Unit sales of product x, aggregated for all stores/states\n",
      "3,049\n",
      "11\n",
      "Unit sales of product x, aggregated for each State\n",
      "9,147\n",
      "12\n",
      "Unit sales of product x, aggregated for each store\n",
      "30,490\n",
      "Total\n",
      "Total\n",
      "42,840 \n",
      "Prize id\n",
      "Prize\n",
      "Amount\n",
      "1A\n",
      "Most accurate point forecasts\n",
      "$25,000\n",
      "2A\n",
      "Second most accurate point forecasts\n",
      "$10,000\n",
      "3A\n",
      "Third most accurate point forecasts\n",
      "  $5,000\n",
      "4A\n",
      "Fourth most accurate point forecasts\n",
      "  $3,000\n",
      "5A\n",
      "Fifth most accurate point forecasts\n",
      "  $2,000\n",
      "6A\n",
      "Most accurate student point forecasts\n",
      "   $5,000\n",
      "\n",
      "Total: M5 Forecasting Competition - Point Forecasts\n",
      "$50,000\n",
      "\n",
      "\n",
      "\n",
      "1B\n",
      "Most precise estimation of the uncertainty distribution\n",
      "$25,000\n",
      "2B\n",
      "Second most precise estimation of the uncertainty distribution\n",
      "$10,000\n",
      "3B\n",
      "Third most precise estimation of the uncertainty distribution\n",
      "  $5,000\n",
      "4B\n",
      "Fourth most precise estimation of the uncertainty distribution\n",
      "  $3,000\n",
      "5B\n",
      "Fifth most precise estimation of the uncertainty distribution\n",
      "  $2,000\n",
      "6B\n",
      "Most precise student estimation of the uncertainty distribution\n",
      "  $5,000\n",
      "\n",
      "Total: M5 Forecasting Competition - Uncertainty Distribution\n",
      "$50,000\n",
      "\n",
      "\n",
      "\n",
      "                  Total: M5 Competition\n",
      "                  Total: M5 Competition\n",
      "     $100,000\n"
     ]
    }
   ],
   "source": [
    "doc= docx.Document('./input/M5-Competitors-Guide-Final-10-March-2020.docx')\n",
    "\n",
    "#全文章を取得\n",
    "for par in doc.paragraphs:\n",
    "    print(par.text)\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "#全テーブルに対して処理を行う。\n",
    "for table in doc.tables:\n",
    "    for row in table.rows:\n",
    "        #print(row.cells[0].text)\n",
    "        for cell in row.cells:\n",
    "            print(cell.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
